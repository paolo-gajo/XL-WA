{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load .tsv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "lang_list = [\n",
    "    # 'ru',\n",
    "    # 'nl',\n",
    "    'it',\n",
    "    # 'pt',\n",
    "    # 'et',\n",
    "    # 'es',\n",
    "    # 'hu',\n",
    "    # 'da',\n",
    "    # 'bg',\n",
    "    # 'sl',\n",
    "]\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "df_dev = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "for lang in lang_list:\n",
    "\n",
    "    df_train_tmp = pd.read_csv(f'/home/pgajo/working/food/src/word_alignment/XL-WA/data/{lang}/train.tsv', sep='\\t', header=None)\n",
    "    df_train_tmp.columns = ['src', 'tgt', 'alignments']\n",
    "    df_train_tmp['lang'] = lang\n",
    "    df_train_tmp['split'] = 'train'\n",
    "\n",
    "    df_dev_tmp = pd.read_csv(f'/home/pgajo/working/food/src/word_alignment/XL-WA/data/{lang}/dev.tsv', sep='\\t', header=None)\n",
    "    df_dev_tmp.columns = ['src', 'tgt', 'alignments']\n",
    "    df_dev_tmp['lang'] = lang\n",
    "    df_dev_tmp['split'] = 'dev'\n",
    "\n",
    "    df_test_tmp = pd.read_csv(f'/home/pgajo/working/food/src/word_alignment/XL-WA/data/{lang}/test.tsv', sep='\\t', header=None)\n",
    "    df_test_tmp.columns = ['src', 'tgt', 'alignments']\n",
    "    df_test_tmp['lang'] = lang\n",
    "    df_test_tmp['split'] = 'test'\n",
    "\n",
    "    # concat train and dev\n",
    "    df_train = pd.concat([df_train, df_train_tmp])\n",
    "    df_dev = pd.concat([df_dev, df_dev_tmp])\n",
    "    df_test = pd.concat([df_test, df_test_tmp])\n",
    "\n",
    "df_train = df_train #[:20]\n",
    "df_dev = df_dev     #[:20]\n",
    "df_test = df_test   #[:20]\n",
    "\n",
    "# display(df)\n",
    "\n",
    "def calculate_spans(sentence):\n",
    "    spans = []\n",
    "    start = 0\n",
    "    for word in sentence.split():\n",
    "        end = start + len(word)\n",
    "        spans.append((start, end))\n",
    "        start = end + 1  # Add 1 for the space character\n",
    "    return spans\n",
    "\n",
    "def convert_alignments(src_sentence, tgt_sentence, alignments):\n",
    "    src_spans = calculate_spans(src_sentence)\n",
    "    tgt_spans = calculate_spans(tgt_sentence)\n",
    "\n",
    "    converted_alignments = []\n",
    "    for alignment in alignments.split():\n",
    "        src_idx, tgt_idx = map(int, alignment.split('-'))\n",
    "        src_span = src_spans[src_idx]\n",
    "        tgt_span = tgt_spans[tgt_idx]\n",
    "        converted_alignments.append(((src_span[0],src_span[1]),(tgt_span[0], tgt_span[1])))\n",
    "\n",
    "    return converted_alignments\n",
    "\n",
    "# Adding a new column for span alignments\n",
    "df_train['span_alignments'] = df_train.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "df_dev['span_alignments'] = df_dev.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "df_test['span_alignments'] = df_test.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "\n",
    "df_dict = {\n",
    "    'train': df_train,\n",
    "    'dev': df_dev,\n",
    "    'test': df_test,\n",
    "}\n",
    "\n",
    "# Now df contains a new column 'span_alignments' with the converted alignments\n",
    "display(df_train)\n",
    "display(df_dev)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_id = '-'.join(df_train['lang'].value_counts().keys())\n",
    "lang_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "# model_name = 'microsoft/mdeberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(linewidth=10000)\n",
    "\n",
    "df_dict_formatted = {\n",
    "    'train': [],\n",
    "    'dev': [],\n",
    "    'test': [],\n",
    "}\n",
    "\n",
    "# create a word_src_j -> sentence_tgt_i dataset\n",
    "for key in df_dict.keys():\n",
    "    tgt_sentences = df_dict[key]['tgt'].tolist()\n",
    "    src_sentences = df_dict[key]['src'].tolist()\n",
    "    src_split_sentences = [sentence.split() for sentence in df_dict[key]['src'].tolist()]\n",
    "    max_num_tokens = 0\n",
    "    for i, sentence_src in enumerate(src_split_sentences):\n",
    "        alignments = df_dict[key]['span_alignments'].to_list()[i]\n",
    "        # print('sentence alignments', alignments)\n",
    "        for j, alignment in enumerate(alignments):\n",
    "            entry = {}\n",
    "            # print('word alignment', alignment)\n",
    "            src_start = alignment[0][0]\n",
    "            # print('src_start', src_start)\n",
    "            src_end = alignment[0][1]\n",
    "            # print('src_end', src_end)\n",
    "            tgt_start = alignment[1][0]\n",
    "            # print('tgt_start', tgt_start)\n",
    "            tgt_end = alignment[1][1]\n",
    "            # print('tgt_end', tgt_end)\n",
    "            entry['id_sentence'] = i\n",
    "            entry['id_alignment'] = j\n",
    "            entry['query'] = src_sentences[i][src_start:src_end]\n",
    "            entry['context'] = tgt_sentences[i]\n",
    "            entry['answer'] = tgt_sentences[i][tgt_start:tgt_end]\n",
    "            entry['answer_start'] = tgt_start\n",
    "            entry['answer_end'] = tgt_end\n",
    "            char_check = entry['context'][entry['answer_start']:entry['answer_end']]\n",
    "            query_encoding = tokenizer(entry['query'])\n",
    "            context_encoding = tokenizer(entry['context'])\n",
    "            entry['answer_start_token'] = context_encoding.char_to_token(entry['answer_start']) + len(query_encoding['input_ids']) - 1\n",
    "            entry['answer_end_token'] = context_encoding.char_to_token(entry['answer_end']-1) + len(query_encoding['input_ids'])\n",
    "            \n",
    "            input_encoding = tokenizer(entry['query'], entry['context'],\n",
    "                                    padding='max_length',\n",
    "                                    max_length=max_length,\n",
    "                                    )\n",
    "\n",
    "            # print(input_encoding['token_type_ids'])\n",
    "            \n",
    "            input_encoding_no_pad = tokenizer(entry['query'], entry['context'])\n",
    "            \n",
    "            if len(input_encoding_no_pad['input_ids']) > max_num_tokens:\n",
    "                max_num_tokens = len(input_encoding_no_pad['input_ids'])\n",
    "            \n",
    "            token_check = tokenizer.decode(input_encoding['input_ids'][entry['answer_start_token']:entry['answer_end_token']])\n",
    "            if not entry['query']:\n",
    "                print('query missing')\n",
    "            \n",
    "            if not char_check == ''.join((token_check).split()):\n",
    "                print('----------------------------------------------')\n",
    "                print(entry['id_sentence'], entry['id_alignment'])\n",
    "                print('src_sentences[i]', src_sentences[i])\n",
    "                print('query_start', src_start)\n",
    "                print('query_end', src_end)\n",
    "                print(\"entry['query']\", entry['query'])\n",
    "                print(\"entry['context']\", entry['context'])\n",
    "                print(\"entry['answer']\", entry['answer'])\n",
    "                print(\"entry['answer_start']\", entry['answer_start'])\n",
    "                print(\"entry['answer_end']\", entry['answer_end'])\n",
    "                print('########### char_check', char_check)\n",
    "                print('query_encoding', query_encoding)\n",
    "                print('context_encoding', context_encoding)\n",
    "                print(entry['answer_start_token'])\n",
    "                print(entry['answer_end_token'])\n",
    "                print('########### token_check', token_check)\n",
    "                print('########### ''.join((token_check).split())', ''.join((token_check).split()))\n",
    "                print('###########', char_check == ''.join((token_check).split()))\n",
    "\n",
    "            # test if the answer_start_token:answer_end_token is the same as the answer\n",
    "            # print(tokenizer.decode(tokenizer(entry['context'])['input_ids'][entry['answer_start_token']:entry['answer_end_token']]))\n",
    "            \n",
    "            # print(entry)\n",
    "            df_dict_formatted[key].append(entry)\n",
    "    print(f'max_num_tokens {key}', max_num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to Dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_train = Dataset.from_list(df_dict_formatted['train'])\n",
    "dataset_dev = Dataset.from_list(df_dict_formatted['dev'])\n",
    "dataset_test = Dataset.from_list(df_dict_formatted['test'])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'validation': dataset_dev,\n",
    "    'test': dataset_test,\n",
    "    })\n",
    "\n",
    "# print(dataset)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['query'], example['context'], \n",
    "                     padding='max_length',\n",
    "                     max_length=max_length,\n",
    "                     )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'answer_start_token', 'answer_end_token'])\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(tokenized_dataset['train'], batch_size = 32, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(tokenized_dataset['validation'], batch_size = 32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(tokenized_dataset['test'], batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "results_path = f'/home/pgajo/working/food/src/word_alignment/XL-WA/results/{lang_id}'\n",
    "if not os.path.isdir(results_path):\n",
    "    os.mkdir(results_path)\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses, train_f1s, train_exact_matches, train_f1s_squad_evaluate, train_exact_matches_squad_evaluate, train_f1s_squad_datasets, train_exact_matches_squad_datasets = [], [], [], [], [], [], []\n",
    "val_losses, val_f1s, val_exact_matches, val_f1s_squad_evaluate, val_exact_matches_squad_evaluate, val_f1s_squad_datasets, val_exact_matches_squad_datasets = [], [], [], [], [], [], []\n",
    "test_losses, test_f1s, test_exact_matches, test_f1s_squad_evaluate, test_exact_matches_squad_evaluate, test_f1s_squad_datasets, test_exact_matches_squad_datasets = [], [], [], [], [], [], []\n",
    "\n",
    "avg_type = 'micro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=1000)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "# Training setup\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "current_timeanddate = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import os\n",
    "import uuid\n",
    "from evaluate import load\n",
    "squad_metric_evaluate = load(\"squad_v2\")\n",
    "from datasets import load_metric\n",
    "squad_metric_datasets = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track the best model and early stopping\n",
    "best_results_val_squad = 0.0\n",
    "best_model = None\n",
    "early_stopping_counter = 0\n",
    "early_stopping_patience = 2  # Set your patience for early stopping\n",
    "\n",
    "# Initialize DataFrame for storing metrics\n",
    "df = pd.DataFrame()\n",
    "\n",
    "epochs = 10\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    ################################################### Training Phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    train_progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    train_all_preds, train_all_true = [], []\n",
    "    train_qa_preds_evaluate, train_qa_trues_evaluate = [], []\n",
    "    train_qa_preds_datasets, train_qa_trues_datasets = [], []\n",
    "\n",
    "    for batch_idx, batch in train_progress_bar: \n",
    "        optimizer.zero_grad()\n",
    "        inputs = {\n",
    "            \"input_ids\": batch['input_ids'].to(device),\n",
    "            'token_type_ids': batch['token_type_ids'].to(device),\n",
    "            \"attention_mask\": batch['attention_mask'].to(device),\n",
    "            \"start_positions\": batch['answer_start_token'].to(device),\n",
    "            \"end_positions\": batch['answer_end_token'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0].mean()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Update tqdm postfix to display loss\n",
    "        train_progress_bar.set_postfix({'Loss': round(epoch_train_loss / (batch_idx + 1), 4)})\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # set to -10000 any logits in the query (left side of the inputs) so that the model cannot predict those tokens\n",
    "        for i, start_output_logits in enumerate(outputs['start_logits']):\n",
    "            start_output_logits = torch.where(inputs['token_type_ids'][i]!=0, start_output_logits, inputs['token_type_ids'][i]-10000)\n",
    "\n",
    "        start_preds = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_preds = torch.argmax(outputs['end_logits'], dim=1)\n",
    "        \n",
    "        pred_batch = [el for el in zip(start_preds.tolist(), end_preds.tolist())]\n",
    "        true_batch = [el for el in zip(inputs[\"start_positions\"].tolist(), inputs[\"end_positions\"].tolist())]\n",
    "        \n",
    "        train_all_preds.extend(pred_batch)\n",
    "        train_all_true.extend(true_batch)\n",
    "\n",
    "        pred_batch_ids = [str(uuid.uuid4()) for i in range(len(start_preds))]\n",
    "\n",
    "        for i, pair in enumerate(pred_batch):\n",
    "            if pair[0] >= pair[1]:\n",
    "                text_pred = ''\n",
    "            else:\n",
    "                text_pred = tokenizer.decode(inputs['input_ids'][i][pair[0]:pair[1]])\n",
    "                if not isinstance(text_pred, str):\n",
    "                    text_pred = ''\n",
    "            \n",
    "            entry_evaluate = {\n",
    "                'prediction_text': text_pred,\n",
    "                'id': pred_batch_ids[i],\n",
    "                'no_answer_probability': 0\n",
    "            }\n",
    "\n",
    "            entry_datasets = {\n",
    "                'prediction_text': text_pred,\n",
    "                'id': pred_batch_ids[i],\n",
    "                # 'no_answer_probability': 0\n",
    "            }\n",
    "\n",
    "            train_qa_preds_evaluate.append(entry_evaluate)\n",
    "            train_qa_preds_datasets.append(entry_datasets)\n",
    "        \n",
    "        for i, pair in enumerate(true_batch):\n",
    "            text_true = tokenizer.decode(inputs['input_ids'][i][pair[0]:pair[1]])\n",
    "            entry = {\n",
    "                'answers': {\n",
    "                    'answer_start': [true_batch[0][0]],\n",
    "                    'text': [text_true],\n",
    "                    },\n",
    "                    'id': pred_batch_ids[i]\n",
    "                }\n",
    "            \n",
    "            train_qa_trues_evaluate.append(entry)\n",
    "            train_qa_trues_datasets.append(entry)\n",
    "\n",
    "\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)   \n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_pred_flat = [p for pair in train_all_preds for p in pair]\n",
    "    train_true_flat = [t for pair in train_all_true for t in pair]\n",
    "    train_f1 = f1_score(train_true_flat, train_pred_flat, average=avg_type)\n",
    "    train_exact_match = accuracy_score(train_true_flat, train_pred_flat)\n",
    "    train_f1s.append(train_f1)\n",
    "    train_exact_matches.append(train_exact_match)\n",
    "\n",
    "    results_train_squad_evaluate = squad_metric_evaluate.compute(predictions=train_qa_preds_evaluate, references=train_qa_trues_evaluate)\n",
    "    results_train_squad_datasets = squad_metric_datasets.compute(predictions=train_qa_preds_datasets, references=train_qa_trues_datasets)\n",
    "    # print(results_train_squad_evaluate)\n",
    "    # print(results_train_squad_datasets)\n",
    "    train_f1s_squad_evaluate.append(results_train_squad_evaluate['f1'])\n",
    "    train_exact_matches_squad_evaluate.append(results_train_squad_evaluate['exact'])\n",
    "    train_f1s_squad_datasets.append(results_train_squad_datasets['f1'])\n",
    "    train_exact_matches_squad_datasets.append(results_train_squad_datasets['exact_match'])\n",
    "\n",
    "    ################################################### Validation Phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    val_all_preds, val_all_true = [], []\n",
    "    val_qa_preds_evaluate, val_qa_trues_evaluate = [], []\n",
    "    val_qa_preds_datasets, val_qa_trues_datasets = [], []\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(val_loader), total=len(val_loader)): \n",
    "        with torch.inference_mode():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch['input_ids'].to(device),\n",
    "                'token_type_ids': batch['token_type_ids'].to(device),\n",
    "                \"attention_mask\": batch['attention_mask'].to(device),\n",
    "                \"start_positions\": batch['answer_start_token'].to(device),\n",
    "                \"end_positions\": batch['answer_end_token'].to(device),\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0].mean()\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "        # set to -10000 any logits in the query (left side of the inputs) so that the model cannot predict those tokens\n",
    "        for i, start_output_logits in enumerate(outputs['start_logits']):\n",
    "            start_output_logits = torch.where(inputs['token_type_ids'][i]!=0, start_output_logits, inputs['token_type_ids'][i]-10000)\n",
    "\n",
    "        start_preds = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_preds = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        pred_batch = [el for el in zip(start_preds.tolist(), end_preds.tolist())]\n",
    "        true_batch = [el for el in zip(inputs[\"start_positions\"].tolist(), inputs[\"end_positions\"].tolist())]\n",
    "\n",
    "        val_all_preds.extend(pred_batch)\n",
    "        val_all_true.extend(true_batch)\n",
    "\n",
    "        pred_batch_ids = [str(uuid.uuid4()) for i in range(len(start_preds))]\n",
    "\n",
    "        for i, pair in enumerate(pred_batch):\n",
    "            if pair[0] >= pair[1]:\n",
    "                text_pred = ''\n",
    "            else:\n",
    "                text_pred = tokenizer.decode(inputs['input_ids'][i][pair[0]:pair[1]])\n",
    "                if not isinstance(text_pred, str):\n",
    "                    text_pred = ''\n",
    "            \n",
    "            entry_evaluate = {\n",
    "                'prediction_text': text_pred,\n",
    "                'id': pred_batch_ids[i],\n",
    "                'no_answer_probability': 0\n",
    "            }\n",
    "\n",
    "            entry_datasets = {\n",
    "                'prediction_text': text_pred,\n",
    "                'id': pred_batch_ids[i],\n",
    "                # 'no_answer_probability': 0\n",
    "            }\n",
    "\n",
    "            val_qa_preds_evaluate.append(entry_evaluate)\n",
    "            val_qa_preds_datasets.append(entry_datasets)\n",
    "        \n",
    "        for i, pair in enumerate(true_batch):\n",
    "            text_true = tokenizer.decode(inputs['input_ids'][i][pair[0]:pair[1]])\n",
    "            entry = {\n",
    "                'answers': {\n",
    "                    'answer_start': [true_batch[0][0]],\n",
    "                    'text': [text_true],\n",
    "                    },\n",
    "                    'id': pred_batch_ids[i]\n",
    "                }\n",
    "            \n",
    "            val_qa_trues_evaluate.append(entry)\n",
    "            val_qa_trues_datasets.append(entry)\n",
    "\n",
    "\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_losses.append(epoch_val_loss)   \n",
    "\n",
    "    # Calculate training metrics\n",
    "    val_pred_flat = [p for pair in val_all_preds for p in pair]\n",
    "    val_true_flat = [t for pair in val_all_true for t in pair]\n",
    "    val_f1 = f1_score(val_true_flat, val_pred_flat, average=avg_type)\n",
    "    val_exact_match = accuracy_score(val_true_flat, val_pred_flat)\n",
    "    val_f1s.append(val_f1)\n",
    "    val_exact_matches.append(val_exact_match)\n",
    "\n",
    "    results_val_squad_evaluate = squad_metric_evaluate.compute(predictions=val_qa_preds_evaluate, references=val_qa_trues_evaluate)\n",
    "    results_val_squad_datasets = squad_metric_datasets.compute(predictions=val_qa_preds_datasets, references=val_qa_trues_datasets)\n",
    "    # print(results_val_squad_evaluate)\n",
    "    # print(results_val_squad_datasets)\n",
    "    val_f1s_squad_evaluate.append(results_val_squad_evaluate['f1'])\n",
    "    val_exact_matches_squad_evaluate.append(results_val_squad_evaluate['exact'])\n",
    "    val_f1s_squad_datasets.append(results_val_squad_datasets['f1'])\n",
    "    val_exact_matches_squad_datasets.append(results_val_squad_datasets['exact_match'])\n",
    "\n",
    "    # Log Epoch Metrics\n",
    "    print(\"\\n-------Epoch \", epoch + 1, \n",
    "          \"-------\"\n",
    "          \"\\nTraining Loss:\", train_losses[-1],\n",
    "          f\"\\nTraining F1 {avg_type}:\", train_f1s[-1],\n",
    "          \"\\nTraining Exact Match:\", train_exact_matches[-1],\n",
    "          \"\\nTraining F1 Squad Evaluate:\", results_train_squad_evaluate['f1'],\n",
    "          \"\\nTraining Exact Squad Evaluate:\", results_train_squad_evaluate['exact'],\n",
    "          \"\\nTraining F1 Squad Datasets:\", results_train_squad_datasets['f1'],\n",
    "          \"\\nTraining Exact Squad Datasets:\", results_train_squad_datasets['exact_match'],\n",
    "          \"\\nValidation Loss:\", val_losses[-1],\n",
    "          f\"\\nValidation F1 {avg_type}:\", val_f1s[-1],\n",
    "          \"\\nValidation Exact Match:\", val_exact_matches[-1],\n",
    "          \"\\nValidation F1 Squad Evaluate:\", results_val_squad_evaluate['f1'],\n",
    "          \"\\nValidation Exact Squad Evaluate:\", results_val_squad_evaluate['exact'],\n",
    "          \"\\nValidation F1 Squad Datasets:\", results_val_squad_datasets['f1'],\n",
    "          \"\\nValidation Exact Squad Datasets:\", results_val_squad_datasets['exact_match'],\n",
    "          \"\\nTime: \", (time.time() - epoch_time),\n",
    "          \"\\n-----------------------\",\n",
    "          \"\\n\\n\")\n",
    "    \n",
    "    test_losses.append('')\n",
    "    test_f1s.append('')\n",
    "    test_exact_matches.append('')\n",
    "    test_f1s_squad_evaluate.append('')\n",
    "    test_exact_matches_squad_evaluate.append('')\n",
    "    test_f1s_squad_datasets.append('')\n",
    "    test_exact_matches_squad_datasets.append('')\n",
    "\n",
    "    # Save metrics to DataFrame and CSV\n",
    "    df = pd.DataFrame({\n",
    "        'epoch': range(epoch+1), \n",
    "        'train_loss': train_losses, \n",
    "        f'train_f1_{avg_type}': train_f1s,\n",
    "        'train_exact_match': train_exact_matches,\n",
    "        'train_f1s_squad_evaluate': train_f1s_squad_evaluate,\n",
    "        'train_exact_matches_squad_evaluate': train_exact_matches_squad_evaluate,\n",
    "        'train_f1s_squad_datasets': train_f1s_squad_datasets,\n",
    "        'train_exact_matches_squad_datasets': train_exact_matches_squad_datasets,\n",
    "        'val_loss': val_losses, \n",
    "        f'val_f1_{avg_type}': val_f1s, \n",
    "        'val_exact_match': val_exact_matches,\n",
    "        'val_f1s_squad_evaluate': val_f1s_squad_evaluate,\n",
    "        'val_exact_matches_squad_evaluate': val_exact_matches_squad_evaluate,\n",
    "        'val_f1s_squad_datasets': val_f1s_squad_datasets,\n",
    "        'val_exact_matches_squad_datasets': val_exact_matches_squad_datasets,\n",
    "        'test_loss': test_losses, \n",
    "        f'test_f1_{avg_type}': test_f1s, \n",
    "        'test_exact_match': test_exact_matches,\n",
    "        'test_f1s_squad_evaluate': test_f1s_squad_evaluate,\n",
    "        'test_exact_matches_squad_evaluate': test_exact_matches_squad_evaluate,\n",
    "        'test_f1s_squad_datasets': test_f1s_squad_datasets,\n",
    "        'test_exact_matches_squad_datasets': test_exact_matches_squad_datasets,\n",
    "    })\n",
    "    csv_filename = f\"{current_timeanddate}_{model_name.split('/')[-1]}_metrics.csv\"\n",
    "    df.to_csv(os.path.join(results_path, csv_filename), index=False)\n",
    "\n",
    "    # Check for Best Model and Implement Early Stopping\n",
    "    if results_val_squad_datasets['exact_match'] > best_results_val_squad or results_val_squad_evaluate['exact'] > best_results_val_squad:\n",
    "        best_results_val_squad = max(results_val_squad_datasets['exact_match'], results_val_squad_evaluate['exact'])\n",
    "        del best_model\n",
    "        best_model = model\n",
    "        epochs_best = epoch + 1\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "model.module.push_to_hub(f\"pgajo/{model_name.split('/')[-1]}-xl-wa-{lang_id}-{epochs_best}-epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load best model for testing evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=1000)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    model_exists = False\n",
    "else:\n",
    "    model_exists = True\n",
    "\n",
    "if model_exists:\n",
    "    pass\n",
    "else:\n",
    "    # load previously saved model\n",
    "    # model_name = \"pgajo/bert-base-multilingual-cased-xl-wa-it-es-9-epochs\"\n",
    "    # model_name = 'pgajo/mdeberta-v3-base-xl-wa-en-it-10-epochs'\n",
    "    # model_name = 'pgajo/bert-base-multilingual-cased-xl-wa-it-10-epochs'\n",
    "    # model_name = 'pgajo/bert-base-multilingual-cased-xl-wa-es-9-epochs'\n",
    "    model_name = 'pgajo/bert-base-multilingual-cased-xl-wa-it-5-epochs'\n",
    "    best_model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "    best_model = torch.nn.DataParallel(best_model)\n",
    "\n",
    "# Training setup\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "current_timeanddate = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import os\n",
    "import uuid\n",
    "from evaluate import load\n",
    "squad_metric_evaluate = load(\"squad_v2\")\n",
    "from datasets import load_metric\n",
    "squad_metric_datasets = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    df_exists = False\n",
    "else:\n",
    "    df_exists = True\n",
    "\n",
    "if df_exists:\n",
    "    pass\n",
    "else:\n",
    "    csv_filename = \"/home/pgajo/working/food/src/word_alignment/XL-WA/results/it/20240117-125659_bert-base-multilingual-cased_metrics.csv\"\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Testing Phase\n",
    "best_model.eval()\n",
    "epoch_test_loss = 0\n",
    "test_all_preds, test_all_true = [], []\n",
    "test_qa_preds, test_qa_trues = [], []\n",
    "for batch_idx, batch in tqdm(enumerate(test_loader), total=len(test_loader)): \n",
    "    with torch.inference_mode():\n",
    "        inputs = {\n",
    "            \"input_ids\": batch['input_ids'].to(device),\n",
    "            'token_type_ids': batch['token_type_ids'].to(device),\n",
    "            \"attention_mask\": batch['attention_mask'].to(device),\n",
    "            \"start_positions\": batch['answer_start_token'].to(device),\n",
    "            \"end_positions\": batch['answer_end_token'].to(device),\n",
    "        }\n",
    "\n",
    "        for i, tensor_input_ids in enumerate(inputs['input_ids']):\n",
    "            tensor_input_ids = torch.where(inputs['token_type_ids'][i]!=0, tensor_input_ids, inputs['token_type_ids'][i]-10000)\n",
    "\n",
    "        outputs = best_model(**inputs)\n",
    "        loss = outputs[0].mean()\n",
    "        epoch_test_loss += loss.item()\n",
    "\n",
    "    # Collect predictions and true testues for metrics\n",
    "    start_preds = torch.argmax(outputs['start_logits'], dim=1)\n",
    "    end_preds = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "    start_preds = torch.argmax(outputs['start_logits'], dim=1)\n",
    "    end_preds = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "    pred_batch = [el for el in zip(start_preds.tolist(), end_preds.tolist())]\n",
    "    true_batch = [el for el in zip(inputs[\"start_positions\"].tolist(), inputs[\"end_positions\"].tolist())]\n",
    "\n",
    "    test_all_preds.extend(pred_batch)\n",
    "    test_all_true.extend(true_batch)\n",
    "\n",
    "    pred_batch_ids = [str(uuid.uuid4()) for i in range(len(start_preds))]\n",
    "\n",
    "    for i, pair in enumerate(pred_batch):\n",
    "        if pair[0] >= pair[1]:\n",
    "            text_pred = ''\n",
    "        else:\n",
    "            text_pred = tokenizer.decode(inputs['input_ids'][i][pair[0]:pair[1]])\n",
    "            if not isinstance(text_pred, str):\n",
    "                text_pred = ''\n",
    "        entry = {\n",
    "            'prediction_text': text_pred,\n",
    "            'id': pred_batch_ids[i],\n",
    "            'no_answer_probability': 0\n",
    "        }\n",
    "\n",
    "        test_qa_preds.append(entry)\n",
    "            \n",
    "        for i, pair in enumerate(true_batch):\n",
    "            text_true = tokenizer.decode(inputs['input_ids'][i][pair[0]:pair[1]])\n",
    "            entry = {\n",
    "                'answers': {\n",
    "                    'answer_start': [true_batch[0][0]],\n",
    "                    'text': [text_true],\n",
    "                    },\n",
    "                    'id': pred_batch_ids[i]\n",
    "                }\n",
    "                \n",
    "        test_qa_trues.append(entry)\n",
    "\n",
    "results_test_squad_evaluate = squad_metric_evaluate.compute(predictions=test_qa_preds, references=test_qa_trues)\n",
    "results_test_squad_datasets = squad_metric_datasets.compute(predictions=test_qa_preds, references=test_qa_trues)\n",
    "\n",
    "epoch_test_loss /= len(test_loader)\n",
    "test_losses.append(epoch_test_loss)\n",
    "\n",
    "# Calculate testidation metrics\n",
    "test_pred_flat = [p for pair in test_all_preds for p in pair]\n",
    "test_true_flat = [t for pair in test_all_true for t in pair]\n",
    "\n",
    "test_f1 = f1_score(test_true_flat, test_pred_flat, average=avg_type)\n",
    "test_exact_match = accuracy_score(test_true_flat, test_pred_flat)\n",
    "test_f1s.append(test_f1)\n",
    "test_exact_matches.append(test_exact_match)\n",
    "test_f1s_squad_evaluate.append(results_test_squad_evaluate['f1'])\n",
    "test_exact_matches_squad_evaluate.append(results_test_squad_evaluate['exact'])\n",
    "test_f1s_squad_datasets.append(results_test_squad_datasets['f1'])\n",
    "test_exact_matches_squad_datasets.append(results_test_squad_datasets['exact_match'])\n",
    "\n",
    "# Log Epoch Metrics\n",
    "print(\"\\nTest Loss:\", test_losses[-1],\n",
    "        f\"\\nTest F1 {avg_type}:\", test_f1s[-1],\n",
    "        \"\\Test Exact Match:\", test_exact_matches[-1],\n",
    "        \"\\nTest F1 Squad Evaluate:\", results_test_squad_evaluate['f1'],\n",
    "        \"\\nTest Exact Squad Evaluate:\", results_test_squad_evaluate['exact'],\n",
    "        \"\\nTest F1 Squad Datasets:\", results_test_squad_datasets['f1'],\n",
    "        \"\\nTest Exact Squad Datasets:\", results_test_squad_datasets['exact_match'],\n",
    "        \"\\n-----------------------\",\n",
    "        \"\\n\\n\")\n",
    "\n",
    "train_losses.append('')\n",
    "train_f1s.append('')\n",
    "train_exact_matches.append('')\n",
    "train_f1s_squad_evaluate.append('')\n",
    "train_exact_matches_squad_evaluate.append('')\n",
    "train_f1s_squad_datasets.append('')\n",
    "train_exact_matches_squad_datasets.append('')\n",
    "val_losses.append('')\n",
    "val_f1s.append('')\n",
    "val_exact_matches.append('')\n",
    "val_f1s_squad_evaluate.append('')\n",
    "val_exact_matches_squad_evaluate.append('')\n",
    "val_f1s_squad_datasets.append('')\n",
    "val_exact_matches_squad_datasets.append('')\n",
    "\n",
    "# Save metrics to DataFrame and CSV\n",
    "dict_df = {\n",
    "    # 'epoch': [el for el in range(int(model_name.split('-')[-2])+1)] + [epochs_best], \n",
    "    'epoch': model_name.split('/')[-1],\n",
    "    'train_loss': train_losses[-1],\n",
    "    f'train_f1_{avg_type}': train_f1s[-1],\n",
    "    'train_exact_match': train_exact_matches[-1],\n",
    "    'train_f1s_squad_evaluate': train_f1s_squad_evaluate[-1],\n",
    "    'train_exact_matches_squad_evaluate': train_exact_matches_squad_evaluate[-1],\n",
    "    'train_f1s_squad_datasets': train_f1s_squad_datasets[-1],\n",
    "    'train_exact_matches_squad_datasets': train_exact_matches_squad_datasets[-1],\n",
    "    'val_loss': val_losses[-1],\n",
    "    f'val_f1_{avg_type}': val_f1s[-1],\n",
    "    'val_exact_match': val_exact_matches[-1],\n",
    "    'val_f1s_squad_evaluate': val_f1s_squad_evaluate[-1],\n",
    "    'val_exact_matches_squad_evaluate': val_exact_matches_squad_evaluate[-1],\n",
    "    'val_f1s_squad_datasets': val_f1s_squad_datasets[-1],\n",
    "    'val_exact_matches_squad_datasets': val_exact_matches_squad_datasets[-1],\n",
    "    'test_loss': test_losses[-1],\n",
    "    f'test_f1_{avg_type}': test_f1s[-1],\n",
    "    'test_exact_match': test_exact_matches[-1],\n",
    "    'test_f1s_squad_evaluate': test_f1s_squad_evaluate[-1],\n",
    "    'test_exact_matches_squad_evaluate': test_exact_matches_squad_evaluate[-1],\n",
    "    'test_f1s_squad_datasets': test_f1s_squad_datasets[-1],\n",
    "    'test_exact_matches_squad_datasets': test_exact_matches_squad_datasets[-1],\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(dict_df, index=[0])])\n",
    "\n",
    "df.to_csv(os.path.join(results_path, csv_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
