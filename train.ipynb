{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/pgajo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface and push model to hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>alignments</th>\n",
       "      <th>lang</th>\n",
       "      <th>split</th>\n",
       "      <th>span_alignments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adding the concepts of \" intermediary \" and \" ...</td>\n",
       "      <td>Aggiungere i concetti di \" intermediario \" e \"...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 6), (0, 10)), ((7, 10), (11, 12)), ((11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr President , the Theato report on the Commis...</td>\n",
       "      <td>Signor Presidente , la relazione Theato sul Li...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 5-4 4-5 6-6 7-6 11-7 10-8 9-9 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 2), (0, 6)), ((3, 12), (7, 17)), ((13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This means that the Annual Report this year co...</td>\n",
       "      <td>Ciò significa che la relazione annuale di ques...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 5-4 4-5 6-7 7-8 8-9 9-10 10-11...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 4), (0, 3)), ((5, 10), (4, 13)), ((11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the Parliament , the Council and the Commis...</td>\n",
       "      <td>Se il Parlamento , il Consiglio e la Commissio...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 2), (0, 2)), ((3, 6), (3, 5)), ((7, 17),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In our view , the Treaty of the European Commu...</td>\n",
       "      <td>A nostro avviso , il Trattato della Comunità e...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-6 9-7 8-8 10-9 1...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 2), (0, 1)), ((3, 6), (2, 8)), ((7, 11),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Such a programme was supported by the Commissi...</td>\n",
       "      <td>Este programa fue apoyado por la Comisión en l...</td>\n",
       "      <td>0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 12-9 11-10...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 4), (0, 4)), ((7, 16), (5, 13)), ((17, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Clearly , the budget for external actions has ...</td>\n",
       "      <td>Evidentemente , el presupuesto de acciones ext...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 6-5 5-6 8-7 7-8 9-9 10-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 7), (0, 13)), ((8, 9), (14, 15)), ((10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Unfortunately , neither of the parties has put...</td>\n",
       "      <td>Lamentablemente , ninguna de las partes ha pue...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 11-9 10-11...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 13), (0, 15)), ((14, 15), (16, 17)), ((1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Our other neighbour , Russia , has many proble...</td>\n",
       "      <td>Nuestro otro vecino , Rusia , tiene muchos pro...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 3), (0, 7)), ((4, 9), (8, 12)), ((10, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>The recent creation of the Competitiveness Cou...</td>\n",
       "      <td>La reciente creación del Consejo de Competitiv...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-3 6-4 5-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 3), (0, 2)), ((4, 10), (3, 11)), ((11, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2004 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src  \\\n",
       "0     Adding the concepts of \" intermediary \" and \" ...   \n",
       "1     Mr President , the Theato report on the Commis...   \n",
       "2     This means that the Annual Report this year co...   \n",
       "3     If the Parliament , the Council and the Commis...   \n",
       "4     In our view , the Treaty of the European Commu...   \n",
       "...                                                 ...   \n",
       "997   Such a programme was supported by the Commissi...   \n",
       "998   Clearly , the budget for external actions has ...   \n",
       "999   Unfortunately , neither of the parties has put...   \n",
       "1000  Our other neighbour , Russia , has many proble...   \n",
       "1001  The recent creation of the Competitiveness Cou...   \n",
       "\n",
       "                                                    tgt  \\\n",
       "0     Aggiungere i concetti di \" intermediario \" e \"...   \n",
       "1     Signor Presidente , la relazione Theato sul Li...   \n",
       "2     Ciò significa che la relazione annuale di ques...   \n",
       "3     Se il Parlamento , il Consiglio e la Commissio...   \n",
       "4     A nostro avviso , il Trattato della Comunità e...   \n",
       "...                                                 ...   \n",
       "997   Este programa fue apoyado por la Comisión en l...   \n",
       "998   Evidentemente , el presupuesto de acciones ext...   \n",
       "999   Lamentablemente , ninguna de las partes ha pue...   \n",
       "1000  Nuestro otro vecino , Rusia , tiene muchos pro...   \n",
       "1001  La reciente creación del Consejo de Competitiv...   \n",
       "\n",
       "                                             alignments lang  split  \\\n",
       "0     0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...   it  train   \n",
       "1     0-0 1-1 2-2 3-3 5-4 4-5 6-6 7-6 11-7 10-8 9-9 ...   it  train   \n",
       "2     0-0 1-1 2-2 3-3 5-4 4-5 6-7 7-8 8-9 9-10 10-11...   it  train   \n",
       "3     0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...   it  train   \n",
       "4     0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-6 9-7 8-8 10-9 1...   it  train   \n",
       "...                                                 ...  ...    ...   \n",
       "997   0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 12-9 11-10...   es  train   \n",
       "998   0-0 1-1 2-2 3-3 4-4 6-5 5-6 8-7 7-8 9-9 10-10 ...   es  train   \n",
       "999   0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 11-9 10-11...   es  train   \n",
       "1000  0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...   es  train   \n",
       "1001  0-0 1-1 2-2 3-3 4-3 6-4 5-6 7-7 8-8 9-9 10-10 ...   es  train   \n",
       "\n",
       "                                        span_alignments  \n",
       "0     [((0, 6), (0, 10)), ((7, 10), (11, 12)), ((11,...  \n",
       "1     [((0, 2), (0, 6)), ((3, 12), (7, 17)), ((13, 1...  \n",
       "2     [((0, 4), (0, 3)), ((5, 10), (4, 13)), ((11, 1...  \n",
       "3     [((0, 2), (0, 2)), ((3, 6), (3, 5)), ((7, 17),...  \n",
       "4     [((0, 2), (0, 1)), ((3, 6), (2, 8)), ((7, 11),...  \n",
       "...                                                 ...  \n",
       "997   [((0, 4), (0, 4)), ((7, 16), (5, 13)), ((17, 2...  \n",
       "998   [((0, 7), (0, 13)), ((8, 9), (14, 15)), ((10, ...  \n",
       "999   [((0, 13), (0, 15)), ((14, 15), (16, 17)), ((1...  \n",
       "1000  [((0, 3), (0, 7)), ((4, 9), (8, 12)), ((10, 19...  \n",
       "1001  [((0, 3), (0, 2)), ((4, 10), (3, 11)), ((11, 1...  \n",
       "\n",
       "[2004 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>alignments</th>\n",
       "      <th>lang</th>\n",
       "      <th>split</th>\n",
       "      <th>span_alignments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With a surface of 84 km² it is the largest nat...</td>\n",
       "      <td>Con una superficie di 84 km² è il più grande l...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 7-6 8-7 9-8 9-9 11-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 4), (0, 3)), ((5, 6), (4, 7)), ((7, 14),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The objectives of Europe 2020 shape the priori...</td>\n",
       "      <td>Gli obiettivi dell' agenda di Lisbona danno fo...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-3 3-4 4-4 3-5 4-5 5-6 5-7 6-...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 3), (0, 3)), ((4, 14), (4, 13)), ((15, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dengue can also be transmitted via infected bl...</td>\n",
       "      <td>La dengue può essere trasmessa anche tramite i...</td>\n",
       "      <td>0-0 0-1 1-2 3-3 4-4 2-5 5-6 7-9 8-9 7-10 8-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 6), (0, 2)), ((0, 6), (3, 9)), ((7, 10),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The result of the second season of the program...</td>\n",
       "      <td>Il risultato della seconda edizione del progra...</td>\n",
       "      <td>0-0 1-1 2-2 3-2 4-3 5-4 6-5 7-5 8-6 9-7 9-8 10...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 3), (0, 2)), ((4, 10), (3, 12)), ((11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teams participating in the championship are fo...</td>\n",
       "      <td>Le squadre che partecipano al campionato sono ...</td>\n",
       "      <td>0-0 0-1 1-2 1-3 2-4 3-4 4-5 5-6 6-7 7-8 8-9 10...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 5), (0, 2)), ((0, 5), (3, 10)), ((6, 19)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Grabowski was in charge of the first Esperanto...</td>\n",
       "      <td>Grabowski se encargó de las primeras clases de...</td>\n",
       "      <td>0-0 1-1 2-1 3-1 1-2 2-2 3-2 4-3 5-4 6-5 8-6 7-...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 9), (0, 9)), ((10, 13), (10, 12)), ((14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>A practical example of an information request ...</td>\n",
       "      <td>Un ejemplo práctico de una solicitud de inform...</td>\n",
       "      <td>0-0 2-1 1-2 3-3 4-4 6-5 5-6 5-7 7-8 8-9 9-10 1...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 1), (0, 2)), ((12, 19), (3, 10)), ((2, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The state's capital is Baton Rouge , and its l...</td>\n",
       "      <td>Su capital es Baton Rouge y su ciudad más pobl...</td>\n",
       "      <td>0-0 2-1 3-2 4-3 5-4 7-5 8-6 10-7 9-8 9-9 6-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 3), (0, 2)), ((12, 19), (3, 10)), ((20, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Soviet people well know what a terrible thing ...</td>\n",
       "      <td>El pueblo soviético sabe muy bien cuán terribl...</td>\n",
       "      <td>1-0 1-1 0-2 3-3 2-5 4-6 6-7 9-8 8-9 8-10 10-11</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((7, 13), (0, 2)), ((7, 13), (3, 9)), ((0, 6)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>A special Commission for Financial and Adminis...</td>\n",
       "      <td>También se ha establecido una comisión especia...</td>\n",
       "      <td>12-0 11-1 13-1 11-3 13-3 0-4 2-5 1-6 3-10 7-11...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((74, 78), (0, 7)), ((71, 73), (8, 10)), ((79...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   src  \\\n",
       "0    With a surface of 84 km² it is the largest nat...   \n",
       "1    The objectives of Europe 2020 shape the priori...   \n",
       "2    Dengue can also be transmitted via infected bl...   \n",
       "3    The result of the second season of the program...   \n",
       "4    Teams participating in the championship are fo...   \n",
       "..                                                 ...   \n",
       "100  Grabowski was in charge of the first Esperanto...   \n",
       "101  A practical example of an information request ...   \n",
       "102  The state's capital is Baton Rouge , and its l...   \n",
       "103  Soviet people well know what a terrible thing ...   \n",
       "104  A special Commission for Financial and Adminis...   \n",
       "\n",
       "                                                   tgt  \\\n",
       "0    Con una superficie di 84 km² è il più grande l...   \n",
       "1    Gli obiettivi dell' agenda di Lisbona danno fo...   \n",
       "2    La dengue può essere trasmessa anche tramite i...   \n",
       "3    Il risultato della seconda edizione del progra...   \n",
       "4    Le squadre che partecipano al campionato sono ...   \n",
       "..                                                 ...   \n",
       "100  Grabowski se encargó de las primeras clases de...   \n",
       "101  Un ejemplo práctico de una solicitud de inform...   \n",
       "102  Su capital es Baton Rouge y su ciudad más pobl...   \n",
       "103  El pueblo soviético sabe muy bien cuán terribl...   \n",
       "104  También se ha establecido una comisión especia...   \n",
       "\n",
       "                                            alignments lang split  \\\n",
       "0    0-0 1-1 2-2 3-3 4-4 5-5 7-6 8-7 9-8 9-9 11-10 ...   it   dev   \n",
       "1    0-0 1-1 2-2 3-3 4-3 3-4 4-4 3-5 4-5 5-6 5-7 6-...   it   dev   \n",
       "2    0-0 0-1 1-2 3-3 4-4 2-5 5-6 7-9 8-9 7-10 8-10 ...   it   dev   \n",
       "3    0-0 1-1 2-2 3-2 4-3 5-4 6-5 7-5 8-6 9-7 9-8 10...   it   dev   \n",
       "4    0-0 0-1 1-2 1-3 2-4 3-4 4-5 5-6 6-7 7-8 8-9 10...   it   dev   \n",
       "..                                                 ...  ...   ...   \n",
       "100  0-0 1-1 2-1 3-1 1-2 2-2 3-2 4-3 5-4 6-5 8-6 7-...   es   dev   \n",
       "101  0-0 2-1 1-2 3-3 4-4 6-5 5-6 5-7 7-8 8-9 9-10 1...   es   dev   \n",
       "102  0-0 2-1 3-2 4-3 5-4 7-5 8-6 10-7 9-8 9-9 6-10 ...   es   dev   \n",
       "103     1-0 1-1 0-2 3-3 2-5 4-6 6-7 9-8 8-9 8-10 10-11   es   dev   \n",
       "104  12-0 11-1 13-1 11-3 13-3 0-4 2-5 1-6 3-10 7-11...   es   dev   \n",
       "\n",
       "                                       span_alignments  \n",
       "0    [((0, 4), (0, 3)), ((5, 6), (4, 7)), ((7, 14),...  \n",
       "1    [((0, 3), (0, 3)), ((4, 14), (4, 13)), ((15, 1...  \n",
       "2    [((0, 6), (0, 2)), ((0, 6), (3, 9)), ((7, 10),...  \n",
       "3    [((0, 3), (0, 2)), ((4, 10), (3, 12)), ((11, 1...  \n",
       "4    [((0, 5), (0, 2)), ((0, 5), (3, 10)), ((6, 19)...  \n",
       "..                                                 ...  \n",
       "100  [((0, 9), (0, 9)), ((10, 13), (10, 12)), ((14,...  \n",
       "101  [((0, 1), (0, 2)), ((12, 19), (3, 10)), ((2, 1...  \n",
       "102  [((0, 3), (0, 2)), ((12, 19), (3, 10)), ((20, ...  \n",
       "103  [((7, 13), (0, 2)), ((7, 13), (3, 9)), ((0, 6)...  \n",
       "104  [((74, 78), (0, 7)), ((71, 73), (8, 10)), ((79...  \n",
       "\n",
       "[208 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_it = pd.read_csv('/home/pgajo/working/food/XL-WA/data/it/train.tsv', sep='\\t', header=None)\n",
    "df_train_it.columns = ['src', 'tgt', 'alignments']\n",
    "df_train_it['lang'] = 'it'\n",
    "df_train_it['split'] = 'train'\n",
    "df_train_es = pd.read_csv('/home/pgajo/working/food/XL-WA/data/es/train.tsv', sep='\\t', header=None)\n",
    "df_train_es.columns = ['src', 'tgt', 'alignments']\n",
    "df_train_es['lang'] = 'es'\n",
    "df_train_es['split'] = 'train'\n",
    "df_train = pd.concat([df_train_it, df_train_es])\n",
    "\n",
    "df_dev_it = pd.read_csv('/home/pgajo/working/food/XL-WA/data/it/dev.tsv', sep='\\t', header=None)\n",
    "df_dev_it.columns = ['src', 'tgt', 'alignments']\n",
    "df_dev_it['lang'] = 'it'\n",
    "df_dev_it['split'] = 'dev'\n",
    "df_dev_es = pd.read_csv('/home/pgajo/working/food/XL-WA/data/es/dev.tsv', sep='\\t', header=None)\n",
    "df_dev_es.columns = ['src', 'tgt', 'alignments']\n",
    "df_dev_es['lang'] = 'es'\n",
    "df_dev_es['split'] = 'dev'\n",
    "df_dev = pd.concat([df_dev_it, df_dev_es])\n",
    "# display(df)\n",
    "\n",
    "def calculate_spans(sentence):\n",
    "    spans = []\n",
    "    start = 0\n",
    "    for word in sentence.split():\n",
    "        end = start + len(word)\n",
    "        spans.append((start, end))\n",
    "        start = end + 1  # Add 1 for the space character\n",
    "    return spans\n",
    "\n",
    "def convert_alignments(src_sentence, tgt_sentence, alignments):\n",
    "    src_spans = calculate_spans(src_sentence)\n",
    "    tgt_spans = calculate_spans(tgt_sentence)\n",
    "\n",
    "    converted_alignments = []\n",
    "    for alignment in alignments.split():\n",
    "        src_idx, tgt_idx = map(int, alignment.split('-'))\n",
    "        src_span = src_spans[src_idx]\n",
    "        tgt_span = tgt_spans[tgt_idx]\n",
    "        converted_alignments.append(((src_span[0],src_span[1]),(tgt_span[0], tgt_span[1])))\n",
    "\n",
    "    return converted_alignments\n",
    "\n",
    "# Adding a new column for span alignments\n",
    "df_train['span_alignments'] = df_train.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "# for i, el in enumerate(df_train['span_alignments'].to_list()):\n",
    "#     print(i, el)\n",
    "#     if i > 10:\n",
    "#         break\n",
    "# for i in range(10):\n",
    "#     print(df_train['span_alignments'][i])\n",
    "df_dev['span_alignments'] = df_dev.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "# for el in df_dev['span_alignments']:\n",
    "#     print(el)\n",
    "# Now df contains a new column 'span_alignments' with the converted alignments\n",
    "display(df_train)\n",
    "display(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'microsoft/mdeberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "292 13\n",
      "src_sentences[i] I declare resumed the session of the European Parliament adjourned on Thursday , 1 February 2001 .\n",
      "query_start 81\n",
      "query_end 82\n",
      "entry['query'] 1\n",
      "entry['context'] Dichiaro ripresa la sessione del Parlamento europeo , interrotta giovedì 1º febbraio 2001 .\n",
      "entry['answer'] 1º\n",
      "entry['answer_start'] 73\n",
      "entry['answer_end'] 75\n",
      "########### char_check 1º\n",
      "query_encoding {'input_ids': [1, 334, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "context_encoding {'input_ids': [1, 60474, 267, 9562, 1419, 64806, 284, 35333, 266, 427, 17665, 269, 17325, 269, 260, 262, 2510, 160165, 119274, 29995, 334, 269, 13626, 32879, 269, 6004, 260, 261, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "22\n",
      "24\n",
      "########### token_check 1o\n",
      "########### .join((token_check).split()) 1o\n",
      "########### False\n",
      "----------------------------------------------\n",
      "1087 16\n",
      "src_sentences[i] Mr President , two years ago the Commission presented two proposed modifications , one of the sixth directive and the other of the Regulation on administrative cooperation .\n",
      "query_start 94\n",
      "query_end 99\n",
      "entry['query'] sixth\n",
      "entry['context'] Señor Presidente , hace dos años la Comisión presentó dos propuestas de modificación , una de la 6ª Directiva y otra del Reglamento sobre cooperación administrativa .\n",
      "entry['answer'] 6ª\n",
      "entry['answer_start'] 97\n",
      "entry['answer_end'] 99\n",
      "########### char_check 6ª\n",
      "query_encoding {'input_ids': [1, 17272, 808, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "context_encoding {'input_ids': [1, 321, 81682, 260, 37324, 260, 262, 7087, 1299, 5548, 284, 1289, 27517, 4964, 554, 1299, 732, 82442, 270, 21315, 932, 260, 262, 574, 270, 284, 571, 263, 17128, 5476, 260, 277, 12986, 427, 79591, 2667, 1616, 65774, 932, 260, 131768, 260, 261, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "29\n",
      "31\n",
      "########### token_check 6a\n",
      "########### .join((token_check).split()) 6a\n",
      "########### False\n",
      "max_num_tokens 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(linewidth=10000)\n",
    "tgt_sentences = df_train['tgt'].tolist()\n",
    "src_sentences = df_train['src'].tolist()\n",
    "src_split_sentences = [sentence.split() for sentence in df_train['src'].tolist()]\n",
    "max_num_tokens = 0\n",
    "dataset_train_list = []\n",
    "# create a word_src_j -> sentence_tgt_i dataset\n",
    "for i, sentence_src in enumerate(src_split_sentences):\n",
    "    alignments = df_train['span_alignments'].to_list()[i]\n",
    "    # print('sentence alignments', alignments)\n",
    "    for j, alignment in enumerate(alignments):\n",
    "        entry = {}\n",
    "        # print('word alignment', alignment)\n",
    "        src_start = alignment[0][0]\n",
    "        # print('src_start', src_start)\n",
    "        src_end = alignment[0][1]\n",
    "        # print('src_end', src_end)\n",
    "        tgt_start = alignment[1][0]\n",
    "        # print('tgt_start', tgt_start)\n",
    "        tgt_end = alignment[1][1]\n",
    "        # print('tgt_end', tgt_end)\n",
    "        entry['id_sentence'] = i\n",
    "        entry['id_alignment'] = j\n",
    "        entry['query'] = src_sentences[i][src_start:src_end]\n",
    "        entry['context'] = tgt_sentences[i]\n",
    "        entry['answer'] = tgt_sentences[i][tgt_start:tgt_end]\n",
    "        entry['answer_start'] = tgt_start\n",
    "        entry['answer_end'] = tgt_end\n",
    "        char_check = entry['context'][entry['answer_start']:entry['answer_end']]\n",
    "        query_encoding = tokenizer(entry['query'])\n",
    "        context_encoding = tokenizer(entry['context'])\n",
    "        entry['answer_start_token'] = context_encoding.char_to_token(entry['answer_start']) + len(query_encoding['input_ids']) - 1\n",
    "        entry['answer_end_token'] = context_encoding.char_to_token(entry['answer_end']-1) + len(query_encoding['input_ids'])\n",
    "        \n",
    "        input_encoding = tokenizer(entry['query'], entry['context'],\n",
    "                                   padding='max_length',\n",
    "                                   max_length=128,\n",
    "                                   truncation=True\n",
    "                                   )\n",
    "        \n",
    "        if len(input_encoding['input_ids']) > max_num_tokens:\n",
    "            max_num_tokens = len(input_encoding['input_ids'])\n",
    "        \n",
    "        token_check = tokenizer.decode(input_encoding['input_ids'][entry['answer_start_token']:entry['answer_end_token']])\n",
    "        if not entry['query']:\n",
    "            print('query missing')\n",
    "        \n",
    "        if not char_check == ''.join((token_check).split()):\n",
    "            print('----------------------------------------------')\n",
    "            print(entry['id_sentence'], entry['id_alignment'])\n",
    "            print('src_sentences[i]', src_sentences[i])\n",
    "            print('query_start', src_start)\n",
    "            print('query_end', src_end)\n",
    "            print(\"entry['query']\", entry['query'])\n",
    "            print(\"entry['context']\", entry['context'])\n",
    "            print(\"entry['answer']\", entry['answer'])\n",
    "            print(\"entry['answer_start']\", entry['answer_start'])\n",
    "            print(\"entry['answer_end']\", entry['answer_end'])\n",
    "            print('########### char_check', char_check)\n",
    "            print('query_encoding', query_encoding)\n",
    "            print('context_encoding', context_encoding)\n",
    "            print(entry['answer_start_token'])\n",
    "            print(entry['answer_end_token'])\n",
    "            print('########### token_check', token_check)\n",
    "            print('########### ''.join((token_check).split())', ''.join((token_check).split()))\n",
    "            print('###########', char_check == ''.join((token_check).split()))\n",
    "\n",
    "        # test if the answer_start_token:answer_end_token is the same as the answer\n",
    "        # print(tokenizer.decode(tokenizer(entry['context'])['input_ids'][entry['answer_start_token']:entry['answer_end_token']]))\n",
    "        \n",
    "        # print(entry)\n",
    "        dataset_train_list.append(entry)\n",
    "print('max_num_tokens', max_num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "0 5\n",
      "src_sentences[i] With a surface of 84 km² it is the largest natural lake in Iceland .\n",
      "query_start 21\n",
      "query_end 24\n",
      "entry['query'] km²\n",
      "entry['context'] Con una superficie di 84 km² è il più grande lago di origine naturale dell' isola .\n",
      "entry['answer'] km²\n",
      "entry['answer_start'] 25\n",
      "entry['answer_end'] 28\n",
      "########### char_check km²\n",
      "query_encoding {'input_ids': [1, 1007, 339, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "context_encoding {'input_ids': [1, 1887, 574, 34252, 266, 302, 10577, 1007, 339, 260, 475, 387, 422, 2853, 4116, 96489, 302, 46244, 26534, 2100, 278, 260, 95806, 260, 261, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "10\n",
      "12\n",
      "########### token_check km2\n",
      "########### .join((token_check).split()) km2\n",
      "########### False\n",
      "----------------------------------------------\n",
      "159 5\n",
      "src_sentences[i] With a surface of 84 km² it is the largest natural lake in Iceland .\n",
      "query_start 21\n",
      "query_end 24\n",
      "entry['query'] km²\n",
      "entry['context'] Con una superficie de 84 km² , es el lago natural más grande de Islandia .\n",
      "entry['answer'] km²\n",
      "entry['answer_start'] 25\n",
      "entry['answer_end'] 28\n",
      "########### char_check km²\n",
      "query_encoding {'input_ids': [1, 1007, 339, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "context_encoding {'input_ids': [1, 1887, 574, 34252, 266, 270, 10577, 1007, 339, 260, 262, 656, 363, 96489, 4927, 1281, 4116, 270, 8259, 540, 260, 261, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "10\n",
      "12\n",
      "########### token_check km2\n",
      "########### .join((token_check).split()) km2\n",
      "########### False\n"
     ]
    }
   ],
   "source": [
    "tgt_sentences = df_dev['tgt'].tolist()\n",
    "src_sentences = df_dev['src'].tolist()\n",
    "src_split_sentences = [sentence.split() for sentence in df_dev['src'].tolist()]\n",
    "\n",
    "dataset_dev_list = []\n",
    "# create a word_src_j -> sentence_tgt_i dataset\n",
    "for i, sentence_src in enumerate(src_split_sentences):\n",
    "    alignments = df_dev['span_alignments'].to_list()[i]\n",
    "    # print('sentence alignments', alignments)\n",
    "    for j, alignment in enumerate(alignments):\n",
    "        entry = {}\n",
    "        # print('word alignment', alignment)\n",
    "        src_start = alignment[0][0]\n",
    "        # print('src_start', src_start)\n",
    "        src_end = alignment[0][1]\n",
    "        # print('src_end', src_end)\n",
    "        tgt_start = alignment[1][0]\n",
    "        # print('tgt_start', tgt_start)\n",
    "        tgt_end = alignment[1][1]\n",
    "        # print('tgt_end', tgt_end)\n",
    "        entry['id_sentence'] = i\n",
    "        entry['id_alignment'] = j\n",
    "        entry['query'] = src_sentences[i][src_start:src_end]\n",
    "        entry['context'] = tgt_sentences[i]\n",
    "        entry['answer'] = tgt_sentences[i][tgt_start:tgt_end]\n",
    "        entry['answer_start'] = tgt_start\n",
    "        entry['answer_end'] = tgt_end\n",
    "        char_check = entry['context'][entry['answer_start']:entry['answer_end']]\n",
    "        query_encoding = tokenizer(entry['query'])\n",
    "        context_encoding = tokenizer(entry['context'])\n",
    "        entry['answer_start_token'] = context_encoding.char_to_token(entry['answer_start']) + len(query_encoding['input_ids']) - 1\n",
    "        entry['answer_end_token'] = context_encoding.char_to_token(entry['answer_end']-1) + len(query_encoding['input_ids'])\n",
    "        \n",
    "        input_encoding = tokenizer(entry['query'], entry['context'],\n",
    "                                   padding='max_length',\n",
    "                                   max_length=128,\n",
    "                                   truncation=True\n",
    "                                   )\n",
    "        token_check = tokenizer.decode(input_encoding['input_ids'][entry['answer_start_token']:entry['answer_end_token']])\n",
    "        if not entry['query']:\n",
    "            print('query missing')\n",
    "        \n",
    "        if not char_check == ''.join((token_check).split()):\n",
    "            print('----------------------------------------------')\n",
    "            print(entry['id_sentence'], entry['id_alignment'])\n",
    "            print('src_sentences[i]', src_sentences[i])\n",
    "            print('query_start', src_start)\n",
    "            print('query_end', src_end)\n",
    "            print(\"entry['query']\", entry['query'])\n",
    "            print(\"entry['context']\", entry['context'])\n",
    "            print(\"entry['answer']\", entry['answer'])\n",
    "            print(\"entry['answer_start']\", entry['answer_start'])\n",
    "            print(\"entry['answer_end']\", entry['answer_end'])\n",
    "            print('########### char_check', char_check)\n",
    "            print('query_encoding', query_encoding)\n",
    "            print('context_encoding', context_encoding)\n",
    "            print(entry['answer_start_token'])\n",
    "            print(entry['answer_end_token'])\n",
    "            print('########### token_check', token_check)\n",
    "            print('########### ''.join((token_check).split())', ''.join((token_check).split()))\n",
    "            print('###########', char_check == ''.join((token_check).split()))\n",
    "\n",
    "        # test if the answer_start_token:answer_end_token is the same as the answer\n",
    "        # print(tokenizer.decode(tokenizer(entry['context'])['input_ids'][entry['answer_start_token']:entry['answer_end_token']]))\n",
    "        \n",
    "        # print(entry)\n",
    "        dataset_dev_list.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ec53eec0ca4feaa6948501affb6f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8a0f4dfaf42158e3fcf8faaaffe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id_sentence', 'id_alignment', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 37245\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id_sentence', 'id_alignment', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3941\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_train = Dataset.from_list(dataset_train_list)\n",
    "dataset_dev = Dataset.from_list(dataset_dev_list)\n",
    "\n",
    "dataset = DatasetDict({'train': dataset_train, 'validation': dataset_dev})\n",
    "# print(dataset)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['query'], example['context'], \n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=128,\n",
    "                     )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'answer_start_token', 'answer_end_token'])\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(tokenized_dataset['train'], batch_size = 32, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(tokenized_dataset['validation'], batch_size = 32, shuffle = True)\n",
    "\n",
    "# inspect the first batch\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     print(batch['input_ids'])\n",
    "#     print(batch['token_type_ids'])\n",
    "#     print(batch['attention_mask'])\n",
    "#     print(batch['answer_start_token'])\n",
    "#     print(batch['answer_end_token'])\n",
    "#     print(batch.keys())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f49daa2b2448ac851443f4d8f910f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 1164 \n",
      "Loss: 3.8 \n",
      "\n",
      "Batch 20 / 1164 \n",
      "Loss: 3.3 \n",
      "\n",
      "Batch 30 / 1164 \n",
      "Loss: 3.0 \n",
      "\n",
      "Batch 40 / 1164 \n",
      "Loss: 2.8 \n",
      "\n",
      "Batch 50 / 1164 \n",
      "Loss: 2.4 \n",
      "\n",
      "Batch 60 / 1164 \n",
      "Loss: 1.9 \n",
      "\n",
      "Batch 70 / 1164 \n",
      "Loss: 0.8 \n",
      "\n",
      "Batch 80 / 1164 \n",
      "Loss: 0.7 \n",
      "\n",
      "Batch 90 / 1164 \n",
      "Loss: 0.6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=1000)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "parallel_model = torch.nn.DataParallel(model)  # Use DataParallel\n",
    "optimizer = torch.optim.AdamW(parallel_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training setup\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "epochs = 3\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print_every = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Set parallel model in train mode\n",
    "    parallel_model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    print(\"############Train############\")\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch['input_ids'].to(device),\n",
    "            'token_type_ids': batch['token_type_ids'].to(device),\n",
    "            \"attention_mask\": batch['attention_mask'].to(device),\n",
    "            \"start_positions\": batch['answer_start_token'].to(device),\n",
    "            \"end_positions\": batch['answer_end_token'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = parallel_model(**inputs)\n",
    "        loss = outputs[0].mean()\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(\"Batch {:} / {:}\".format(batch_idx + 1, len(train_loader)), \"\\nLoss:\", round(loss.item(), 1), \"\\n\")\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    ########## Evaluation ##################\n",
    "    parallel_model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(val_loader), total=len(val_loader)): \n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch['input_ids'].to(device),\n",
    "                'token_type_ids': batch['token_type_ids'].to(device),\n",
    "                \"attention_mask\": batch['attention_mask'].to(device),\n",
    "                \"start_positions\": batch['answer_start_token'].to(device),\n",
    "                \"end_positions\": batch['answer_end_token'].to(device),\n",
    "            }\n",
    "            \n",
    "            outputs = parallel_model(**inputs)\n",
    "            loss = outputs[0].mean()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % print_every == 0:\n",
    "                print(\"Batch {:} / {:}\".format(batch_idx + 1, len(val_loader)), \"\\nLoss:\", round(loss.item(), 1), \"\\n\")\n",
    "\n",
    "    epoch_loss /= len(val_loader)\n",
    "    val_losses.append(epoch_loss)\n",
    "\n",
    "    print(\"\\n-------Epoch \", epoch + 1, \n",
    "          \"-------\"\n",
    "          \"\\nTraining Loss:\", train_losses[-1],\n",
    "          \"\\nValidation Loss:\", val_losses[-1],\n",
    "          \"\\nTime: \", (time.time() - epoch_time),\n",
    "          \"\\n-----------------------\",\n",
    "          \"\\n\\n\")\n",
    "\n",
    "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(f\"pgajo/mdeberta-v3-base-xl-wa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"pgajo/bert-base-multilingual-cased-word-align\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
