{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/pgajo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# login to huggingface and push model to hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>alignments</th>\n",
       "      <th>lang</th>\n",
       "      <th>split</th>\n",
       "      <th>span_alignments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adding the concepts of \" intermediary \" and \" ...</td>\n",
       "      <td>Aggiungere i concetti di \" intermediario \" e \"...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 6), (0, 10)), ((7, 10), (11, 12)), ((11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr President , the Theato report on the Commis...</td>\n",
       "      <td>Signor Presidente , la relazione Theato sul Li...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 5-4 4-5 6-6 7-6 11-7 10-8 9-9 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 2), (0, 6)), ((3, 12), (7, 17)), ((13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This means that the Annual Report this year co...</td>\n",
       "      <td>Ciò significa che la relazione annuale di ques...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 5-4 4-5 6-7 7-8 8-9 9-10 10-11...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 4), (0, 3)), ((5, 10), (4, 13)), ((11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If the Parliament , the Council and the Commis...</td>\n",
       "      <td>Se il Parlamento , il Consiglio e la Commissio...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 2), (0, 2)), ((3, 6), (3, 5)), ((7, 17),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In our view , the Treaty of the European Commu...</td>\n",
       "      <td>A nostro avviso , il Trattato della Comunità e...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-6 9-7 8-8 10-9 1...</td>\n",
       "      <td>it</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 2), (0, 1)), ((3, 6), (2, 8)), ((7, 11),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Such a programme was supported by the Commissi...</td>\n",
       "      <td>Este programa fue apoyado por la Comisión en l...</td>\n",
       "      <td>0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 12-9 11-10...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 4), (0, 4)), ((7, 16), (5, 13)), ((17, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Clearly , the budget for external actions has ...</td>\n",
       "      <td>Evidentemente , el presupuesto de acciones ext...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 6-5 5-6 8-7 7-8 9-9 10-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 7), (0, 13)), ((8, 9), (14, 15)), ((10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Unfortunately , neither of the parties has put...</td>\n",
       "      <td>Lamentablemente , ninguna de las partes ha pue...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 11-9 10-11...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 13), (0, 15)), ((14, 15), (16, 17)), ((1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Our other neighbour , Russia , has many proble...</td>\n",
       "      <td>Nuestro otro vecino , Rusia , tiene muchos pro...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 3), (0, 7)), ((4, 9), (8, 12)), ((10, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>The recent creation of the Competitiveness Cou...</td>\n",
       "      <td>La reciente creación del Consejo de Competitiv...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-3 6-4 5-6 7-7 8-8 9-9 10-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>train</td>\n",
       "      <td>[((0, 3), (0, 2)), ((4, 10), (3, 11)), ((11, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2004 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    src  \\\n",
       "0     Adding the concepts of \" intermediary \" and \" ...   \n",
       "1     Mr President , the Theato report on the Commis...   \n",
       "2     This means that the Annual Report this year co...   \n",
       "3     If the Parliament , the Council and the Commis...   \n",
       "4     In our view , the Treaty of the European Commu...   \n",
       "...                                                 ...   \n",
       "997   Such a programme was supported by the Commissi...   \n",
       "998   Clearly , the budget for external actions has ...   \n",
       "999   Unfortunately , neither of the parties has put...   \n",
       "1000  Our other neighbour , Russia , has many proble...   \n",
       "1001  The recent creation of the Competitiveness Cou...   \n",
       "\n",
       "                                                    tgt  \\\n",
       "0     Aggiungere i concetti di \" intermediario \" e \"...   \n",
       "1     Signor Presidente , la relazione Theato sul Li...   \n",
       "2     Ciò significa che la relazione annuale di ques...   \n",
       "3     Se il Parlamento , il Consiglio e la Commissio...   \n",
       "4     A nostro avviso , il Trattato della Comunità e...   \n",
       "...                                                 ...   \n",
       "997   Este programa fue apoyado por la Comisión en l...   \n",
       "998   Evidentemente , el presupuesto de acciones ext...   \n",
       "999   Lamentablemente , ninguna de las partes ha pue...   \n",
       "1000  Nuestro otro vecino , Rusia , tiene muchos pro...   \n",
       "1001  La reciente creación del Consejo de Competitiv...   \n",
       "\n",
       "                                             alignments lang  split  \\\n",
       "0     0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...   it  train   \n",
       "1     0-0 1-1 2-2 3-3 5-4 4-5 6-6 7-6 11-7 10-8 9-9 ...   it  train   \n",
       "2     0-0 1-1 2-2 3-3 5-4 4-5 6-7 7-8 8-9 9-10 10-11...   it  train   \n",
       "3     0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...   it  train   \n",
       "4     0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-6 9-7 8-8 10-9 1...   it  train   \n",
       "...                                                 ...  ...    ...   \n",
       "997   0-0 2-1 3-2 4-3 5-4 6-5 7-6 8-7 9-8 12-9 11-10...   es  train   \n",
       "998   0-0 1-1 2-2 3-3 4-4 6-5 5-6 8-7 7-8 9-9 10-10 ...   es  train   \n",
       "999   0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 11-9 10-11...   es  train   \n",
       "1000  0-0 1-1 2-2 3-3 4-4 5-5 6-6 7-7 8-8 9-9 10-10 ...   es  train   \n",
       "1001  0-0 1-1 2-2 3-3 4-3 6-4 5-6 7-7 8-8 9-9 10-10 ...   es  train   \n",
       "\n",
       "                                        span_alignments  \n",
       "0     [((0, 6), (0, 10)), ((7, 10), (11, 12)), ((11,...  \n",
       "1     [((0, 2), (0, 6)), ((3, 12), (7, 17)), ((13, 1...  \n",
       "2     [((0, 4), (0, 3)), ((5, 10), (4, 13)), ((11, 1...  \n",
       "3     [((0, 2), (0, 2)), ((3, 6), (3, 5)), ((7, 17),...  \n",
       "4     [((0, 2), (0, 1)), ((3, 6), (2, 8)), ((7, 11),...  \n",
       "...                                                 ...  \n",
       "997   [((0, 4), (0, 4)), ((7, 16), (5, 13)), ((17, 2...  \n",
       "998   [((0, 7), (0, 13)), ((8, 9), (14, 15)), ((10, ...  \n",
       "999   [((0, 13), (0, 15)), ((14, 15), (16, 17)), ((1...  \n",
       "1000  [((0, 3), (0, 7)), ((4, 9), (8, 12)), ((10, 19...  \n",
       "1001  [((0, 3), (0, 2)), ((4, 10), (3, 11)), ((11, 1...  \n",
       "\n",
       "[2004 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>alignments</th>\n",
       "      <th>lang</th>\n",
       "      <th>split</th>\n",
       "      <th>span_alignments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With a surface of 84 km² it is the largest nat...</td>\n",
       "      <td>Con una superficie di 84 km² è il più grande l...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-4 5-5 7-6 8-7 9-8 9-9 11-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 4), (0, 3)), ((5, 6), (4, 7)), ((7, 14),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The objectives of Europe 2020 shape the priori...</td>\n",
       "      <td>Gli obiettivi dell' agenda di Lisbona danno fo...</td>\n",
       "      <td>0-0 1-1 2-2 3-3 4-3 3-4 4-4 3-5 4-5 5-6 5-7 6-...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 3), (0, 3)), ((4, 14), (4, 13)), ((15, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dengue can also be transmitted via infected bl...</td>\n",
       "      <td>La dengue può essere trasmessa anche tramite i...</td>\n",
       "      <td>0-0 0-1 1-2 3-3 4-4 2-5 5-6 7-9 8-9 7-10 8-10 ...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 6), (0, 2)), ((0, 6), (3, 9)), ((7, 10),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The result of the second season of the program...</td>\n",
       "      <td>Il risultato della seconda edizione del progra...</td>\n",
       "      <td>0-0 1-1 2-2 3-2 4-3 5-4 6-5 7-5 8-6 9-7 9-8 10...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 3), (0, 2)), ((4, 10), (3, 12)), ((11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teams participating in the championship are fo...</td>\n",
       "      <td>Le squadre che partecipano al campionato sono ...</td>\n",
       "      <td>0-0 0-1 1-2 1-3 2-4 3-4 4-5 5-6 6-7 7-8 8-9 10...</td>\n",
       "      <td>it</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 5), (0, 2)), ((0, 5), (3, 10)), ((6, 19)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Grabowski was in charge of the first Esperanto...</td>\n",
       "      <td>Grabowski se encargó de las primeras clases de...</td>\n",
       "      <td>0-0 1-1 2-1 3-1 1-2 2-2 3-2 4-3 5-4 6-5 8-6 7-...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 9), (0, 9)), ((10, 13), (10, 12)), ((14,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>A practical example of an information request ...</td>\n",
       "      <td>Un ejemplo práctico de una solicitud de inform...</td>\n",
       "      <td>0-0 2-1 1-2 3-3 4-4 6-5 5-6 5-7 7-8 8-9 9-10 1...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 1), (0, 2)), ((12, 19), (3, 10)), ((2, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>The state's capital is Baton Rouge , and its l...</td>\n",
       "      <td>Su capital es Baton Rouge y su ciudad más pobl...</td>\n",
       "      <td>0-0 2-1 3-2 4-3 5-4 7-5 8-6 10-7 9-8 9-9 6-10 ...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((0, 3), (0, 2)), ((12, 19), (3, 10)), ((20, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Soviet people well know what a terrible thing ...</td>\n",
       "      <td>El pueblo soviético sabe muy bien cuán terribl...</td>\n",
       "      <td>1-0 1-1 0-2 3-3 2-5 4-6 6-7 9-8 8-9 8-10 10-11</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((7, 13), (0, 2)), ((7, 13), (3, 9)), ((0, 6)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>A special Commission for Financial and Adminis...</td>\n",
       "      <td>También se ha establecido una comisión especia...</td>\n",
       "      <td>12-0 11-1 13-1 11-3 13-3 0-4 2-5 1-6 3-10 7-11...</td>\n",
       "      <td>es</td>\n",
       "      <td>dev</td>\n",
       "      <td>[((74, 78), (0, 7)), ((71, 73), (8, 10)), ((79...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   src  \\\n",
       "0    With a surface of 84 km² it is the largest nat...   \n",
       "1    The objectives of Europe 2020 shape the priori...   \n",
       "2    Dengue can also be transmitted via infected bl...   \n",
       "3    The result of the second season of the program...   \n",
       "4    Teams participating in the championship are fo...   \n",
       "..                                                 ...   \n",
       "100  Grabowski was in charge of the first Esperanto...   \n",
       "101  A practical example of an information request ...   \n",
       "102  The state's capital is Baton Rouge , and its l...   \n",
       "103  Soviet people well know what a terrible thing ...   \n",
       "104  A special Commission for Financial and Adminis...   \n",
       "\n",
       "                                                   tgt  \\\n",
       "0    Con una superficie di 84 km² è il più grande l...   \n",
       "1    Gli obiettivi dell' agenda di Lisbona danno fo...   \n",
       "2    La dengue può essere trasmessa anche tramite i...   \n",
       "3    Il risultato della seconda edizione del progra...   \n",
       "4    Le squadre che partecipano al campionato sono ...   \n",
       "..                                                 ...   \n",
       "100  Grabowski se encargó de las primeras clases de...   \n",
       "101  Un ejemplo práctico de una solicitud de inform...   \n",
       "102  Su capital es Baton Rouge y su ciudad más pobl...   \n",
       "103  El pueblo soviético sabe muy bien cuán terribl...   \n",
       "104  También se ha establecido una comisión especia...   \n",
       "\n",
       "                                            alignments lang split  \\\n",
       "0    0-0 1-1 2-2 3-3 4-4 5-5 7-6 8-7 9-8 9-9 11-10 ...   it   dev   \n",
       "1    0-0 1-1 2-2 3-3 4-3 3-4 4-4 3-5 4-5 5-6 5-7 6-...   it   dev   \n",
       "2    0-0 0-1 1-2 3-3 4-4 2-5 5-6 7-9 8-9 7-10 8-10 ...   it   dev   \n",
       "3    0-0 1-1 2-2 3-2 4-3 5-4 6-5 7-5 8-6 9-7 9-8 10...   it   dev   \n",
       "4    0-0 0-1 1-2 1-3 2-4 3-4 4-5 5-6 6-7 7-8 8-9 10...   it   dev   \n",
       "..                                                 ...  ...   ...   \n",
       "100  0-0 1-1 2-1 3-1 1-2 2-2 3-2 4-3 5-4 6-5 8-6 7-...   es   dev   \n",
       "101  0-0 2-1 1-2 3-3 4-4 6-5 5-6 5-7 7-8 8-9 9-10 1...   es   dev   \n",
       "102  0-0 2-1 3-2 4-3 5-4 7-5 8-6 10-7 9-8 9-9 6-10 ...   es   dev   \n",
       "103     1-0 1-1 0-2 3-3 2-5 4-6 6-7 9-8 8-9 8-10 10-11   es   dev   \n",
       "104  12-0 11-1 13-1 11-3 13-3 0-4 2-5 1-6 3-10 7-11...   es   dev   \n",
       "\n",
       "                                       span_alignments  \n",
       "0    [((0, 4), (0, 3)), ((5, 6), (4, 7)), ((7, 14),...  \n",
       "1    [((0, 3), (0, 3)), ((4, 14), (4, 13)), ((15, 1...  \n",
       "2    [((0, 6), (0, 2)), ((0, 6), (3, 9)), ((7, 10),...  \n",
       "3    [((0, 3), (0, 2)), ((4, 10), (3, 12)), ((11, 1...  \n",
       "4    [((0, 5), (0, 2)), ((0, 5), (3, 10)), ((6, 19)...  \n",
       "..                                                 ...  \n",
       "100  [((0, 9), (0, 9)), ((10, 13), (10, 12)), ((14,...  \n",
       "101  [((0, 1), (0, 2)), ((12, 19), (3, 10)), ((2, 1...  \n",
       "102  [((0, 3), (0, 2)), ((12, 19), (3, 10)), ((20, ...  \n",
       "103  [((7, 13), (0, 2)), ((7, 13), (3, 9)), ((0, 6)...  \n",
       "104  [((74, 78), (0, 7)), ((71, 73), (8, 10)), ((79...  \n",
       "\n",
       "[208 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train_it = pd.read_csv('/home/pgajo/working/food/XL-WA/data/it/train.tsv', sep='\\t', header=None)\n",
    "df_train_it.columns = ['src', 'tgt', 'alignments']\n",
    "df_train_it['lang'] = 'it'\n",
    "df_train_it['split'] = 'train'\n",
    "df_train_es = pd.read_csv('/home/pgajo/working/food/XL-WA/data/es/train.tsv', sep='\\t', header=None)\n",
    "df_train_es.columns = ['src', 'tgt', 'alignments']\n",
    "df_train_es['lang'] = 'es'\n",
    "df_train_es['split'] = 'train'\n",
    "df_train = pd.concat([df_train_it, df_train_es])\n",
    "\n",
    "df_dev_it = pd.read_csv('/home/pgajo/working/food/XL-WA/data/it/dev.tsv', sep='\\t', header=None)\n",
    "df_dev_it.columns = ['src', 'tgt', 'alignments']\n",
    "df_dev_it['lang'] = 'it'\n",
    "df_dev_it['split'] = 'dev'\n",
    "df_dev_es = pd.read_csv('/home/pgajo/working/food/XL-WA/data/es/dev.tsv', sep='\\t', header=None)\n",
    "df_dev_es.columns = ['src', 'tgt', 'alignments']\n",
    "df_dev_es['lang'] = 'es'\n",
    "df_dev_es['split'] = 'dev'\n",
    "df_dev = pd.concat([df_dev_it, df_dev_es])\n",
    "# display(df)\n",
    "\n",
    "def calculate_spans(sentence):\n",
    "    spans = []\n",
    "    start = 0\n",
    "    for word in sentence.split():\n",
    "        end = start + len(word)\n",
    "        spans.append((start, end))\n",
    "        start = end + 1  # Add 1 for the space character\n",
    "    return spans\n",
    "\n",
    "def convert_alignments(src_sentence, tgt_sentence, alignments):\n",
    "    src_spans = calculate_spans(src_sentence)\n",
    "    tgt_spans = calculate_spans(tgt_sentence)\n",
    "\n",
    "    converted_alignments = []\n",
    "    for alignment in alignments.split():\n",
    "        src_idx, tgt_idx = map(int, alignment.split('-'))\n",
    "        src_span = src_spans[src_idx]\n",
    "        tgt_span = tgt_spans[tgt_idx]\n",
    "        converted_alignments.append(((src_span[0],src_span[1]),(tgt_span[0], tgt_span[1])))\n",
    "\n",
    "    return converted_alignments\n",
    "\n",
    "# Adding a new column for span alignments\n",
    "df_train['span_alignments'] = df_train.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "# for i, el in enumerate(df_train['span_alignments'].to_list()):\n",
    "#     print(i, el)\n",
    "#     if i > 10:\n",
    "#         break\n",
    "# for i in range(10):\n",
    "#     print(df_train['span_alignments'][i])\n",
    "df_dev['span_alignments'] = df_dev.apply(lambda row: convert_alignments(row[0], row[1], row[2]), axis=1)\n",
    "# for el in df_dev['span_alignments']:\n",
    "#     print(el)\n",
    "# Now df contains a new column 'span_alignments' with the converted alignments\n",
    "display(df_train)\n",
    "display(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'microsoft/mdeberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pgajo/working/food/XL-WA/train.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmagamago.sslmit.unibo.it/home/pgajo/working/food/XL-WA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m entry[\u001b[39m'\u001b[39m\u001b[39manswer_end\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m tgt_end\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmagamago.sslmit.unibo.it/home/pgajo/working/food/XL-WA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m char_check \u001b[39m=\u001b[39m entry[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m][entry[\u001b[39m'\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m'\u001b[39m]:entry[\u001b[39m'\u001b[39m\u001b[39manswer_end\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmagamago.sslmit.unibo.it/home/pgajo/working/food/XL-WA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m query_encoding \u001b[39m=\u001b[39m tokenizer(entry[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmagamago.sslmit.unibo.it/home/pgajo/working/food/XL-WA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m context_encoding \u001b[39m=\u001b[39m tokenizer(entry[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmagamago.sslmit.unibo.it/home/pgajo/working/food/XL-WA/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m entry[\u001b[39m'\u001b[39m\u001b[39manswer_start_token\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m context_encoding\u001b[39m.\u001b[39mchar_to_token(entry[\u001b[39m'\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(query_encoding[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/working/food/food-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2803\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/working/food/food-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2909\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2910\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2911\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2912\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2913\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2914\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2915\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2916\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2917\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2918\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2919\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2920\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2921\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2922\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2923\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2924\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2925\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2926\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2927\u001b[0m     )\n",
      "File \u001b[0;32m~/working/food/food-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2982\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2983\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2984\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2985\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2986\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2987\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2988\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2989\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2990\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2991\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2992\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2993\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2994\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2995\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2996\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2997\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2998\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2999\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3000\u001b[0m )\n",
      "File \u001b[0;32m~/working/food/food-env/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    555\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    574\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    575\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 576\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    577\u001b[0m         batched_input,\n\u001b[1;32m    578\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    579\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    580\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    581\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    582\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    583\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    584\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    585\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    586\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    587\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    588\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    589\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    590\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    591\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    592\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    593\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    596\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/working/food/food-env/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/working/food/food-env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    205\u001b[0m     data: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     n_sequences: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    210\u001b[0m ):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data)\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(encoding, EncodingFast):\n\u001b[1;32m    214\u001b[0m         encoding \u001b[39m=\u001b[39m [encoding]\n",
      "File \u001b[0;32m/usr/lib/python3.8/collections/__init__.py:999\u001b[0m, in \u001b[0;36mUserDict.__init__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {}\n\u001b[1;32m    998\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mdict\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39mdict\u001b[39;49m)\n\u001b[1;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m   1001\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.8/_collections_abc.py:832\u001b[0m, in \u001b[0;36mMutableMapping.update\u001b[0;34m(self, other, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Mapping):\n\u001b[1;32m    831\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m other:\n\u001b[0;32m--> 832\u001b[0m         \u001b[39mself\u001b[39;49m[key] \u001b[39m=\u001b[39m other[key]\n\u001b[1;32m    833\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(other, \u001b[39m\"\u001b[39m\u001b[39mkeys\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m other\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/usr/lib/python3.8/collections/__init__.py:1011\u001b[0m, in \u001b[0;36mUserDict.__setitem__\u001b[0;34m(self, key, item)\u001b[0m\n\u001b[0;32m-> 1011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key, item): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[key] \u001b[39m=\u001b[39m item\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(linewidth=10000)\n",
    "tgt_sentences = df_train['tgt'].tolist()\n",
    "src_sentences = df_train['src'].tolist()\n",
    "src_split_sentences = [sentence.split() for sentence in df_train['src'].tolist()]\n",
    "max_num_tokens = 0\n",
    "dataset_train_list = []\n",
    "# create a word_src_j -> sentence_tgt_i dataset\n",
    "for i, sentence_src in enumerate(src_split_sentences):\n",
    "    alignments = df_train['span_alignments'].to_list()[i]\n",
    "    # print('sentence alignments', alignments)\n",
    "    for j, alignment in enumerate(alignments):\n",
    "        entry = {}\n",
    "        # print('word alignment', alignment)\n",
    "        src_start = alignment[0][0]\n",
    "        # print('src_start', src_start)\n",
    "        src_end = alignment[0][1]\n",
    "        # print('src_end', src_end)\n",
    "        tgt_start = alignment[1][0]\n",
    "        # print('tgt_start', tgt_start)\n",
    "        tgt_end = alignment[1][1]\n",
    "        # print('tgt_end', tgt_end)\n",
    "        entry['id_sentence'] = i\n",
    "        entry['id_alignment'] = j\n",
    "        entry['query'] = src_sentences[i][src_start:src_end]\n",
    "        entry['context'] = tgt_sentences[i]\n",
    "        entry['answer'] = tgt_sentences[i][tgt_start:tgt_end]\n",
    "        entry['answer_start'] = tgt_start\n",
    "        entry['answer_end'] = tgt_end\n",
    "        char_check = entry['context'][entry['answer_start']:entry['answer_end']]\n",
    "        query_encoding = tokenizer(entry['query'])\n",
    "        context_encoding = tokenizer(entry['context'])\n",
    "        entry['answer_start_token'] = context_encoding.char_to_token(entry['answer_start']) + len(query_encoding['input_ids']) - 1\n",
    "        entry['answer_end_token'] = context_encoding.char_to_token(entry['answer_end']-1) + len(query_encoding['input_ids'])\n",
    "        \n",
    "        input_encoding = tokenizer(entry['query'], entry['context'],\n",
    "                                   padding='max_length',\n",
    "                                   max_length=128,\n",
    "                                   truncation=True\n",
    "                                   )\n",
    "        \n",
    "        if len(input_encoding['input_ids']) > max_num_tokens:\n",
    "            max_num_tokens = len(input_encoding['input_ids'])\n",
    "        \n",
    "        token_check = tokenizer.decode(input_encoding['input_ids'][entry['answer_start_token']:entry['answer_end_token']])\n",
    "        if not entry['query']:\n",
    "            print('query missing')\n",
    "        \n",
    "        if not char_check == ''.join((token_check).split()):\n",
    "            print('----------------------------------------------')\n",
    "            print(entry['id_sentence'], entry['id_alignment'])\n",
    "            print('src_sentences[i]', src_sentences[i])\n",
    "            print('query_start', src_start)\n",
    "            print('query_end', src_end)\n",
    "            print(\"entry['query']\", entry['query'])\n",
    "            print(\"entry['context']\", entry['context'])\n",
    "            print(\"entry['answer']\", entry['answer'])\n",
    "            print(\"entry['answer_start']\", entry['answer_start'])\n",
    "            print(\"entry['answer_end']\", entry['answer_end'])\n",
    "            print('########### char_check', char_check)\n",
    "            print('query_encoding', query_encoding)\n",
    "            print('context_encoding', context_encoding)\n",
    "            print(entry['answer_start_token'])\n",
    "            print(entry['answer_end_token'])\n",
    "            print('########### token_check', token_check)\n",
    "            print('########### ''.join((token_check).split())', ''.join((token_check).split()))\n",
    "            print('###########', char_check == ''.join((token_check).split()))\n",
    "\n",
    "        # test if the answer_start_token:answer_end_token is the same as the answer\n",
    "        # print(tokenizer.decode(tokenizer(entry['context'])['input_ids'][entry['answer_start_token']:entry['answer_end_token']]))\n",
    "        \n",
    "        # print(entry)\n",
    "        dataset_train_list.append(entry)\n",
    "print('max_num_tokens', max_num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "0 5\n",
      "src_sentences[i] With a surface of 84 km² it is the largest natural lake in Iceland .\n",
      "query_start 21\n",
      "query_end 24\n",
      "entry['query'] km²\n",
      "entry['context'] Con una superficie di 84 km² è il più grande lago di origine naturale dell' isola .\n",
      "entry['answer'] km²\n",
      "entry['answer_start'] 25\n",
      "entry['answer_end'] 28\n",
      "########### char_check km²\n",
      "query_encoding {'input_ids': [1, 1007, 339, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "context_encoding {'input_ids': [1, 1887, 574, 34252, 266, 302, 10577, 1007, 339, 260, 475, 387, 422, 2853, 4116, 96489, 302, 46244, 26534, 2100, 278, 260, 95806, 260, 261, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "10\n",
      "12\n",
      "########### token_check km2\n",
      "########### .join((token_check).split()) km2\n",
      "########### False\n",
      "----------------------------------------------\n",
      "159 5\n",
      "src_sentences[i] With a surface of 84 km² it is the largest natural lake in Iceland .\n",
      "query_start 21\n",
      "query_end 24\n",
      "entry['query'] km²\n",
      "entry['context'] Con una superficie de 84 km² , es el lago natural más grande de Islandia .\n",
      "entry['answer'] km²\n",
      "entry['answer_start'] 25\n",
      "entry['answer_end'] 28\n",
      "########### char_check km²\n",
      "query_encoding {'input_ids': [1, 1007, 339, 2], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "context_encoding {'input_ids': [1, 1887, 574, 34252, 266, 270, 10577, 1007, 339, 260, 262, 656, 363, 96489, 4927, 1281, 4116, 270, 8259, 540, 260, 261, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "10\n",
      "12\n",
      "########### token_check km2\n",
      "########### .join((token_check).split()) km2\n",
      "########### False\n"
     ]
    }
   ],
   "source": [
    "tgt_sentences = df_dev['tgt'].tolist()\n",
    "src_sentences = df_dev['src'].tolist()\n",
    "src_split_sentences = [sentence.split() for sentence in df_dev['src'].tolist()]\n",
    "\n",
    "dataset_dev_list = []\n",
    "# create a word_src_j -> sentence_tgt_i dataset\n",
    "for i, sentence_src in enumerate(src_split_sentences):\n",
    "    alignments = df_dev['span_alignments'].to_list()[i]\n",
    "    # print('sentence alignments', alignments)\n",
    "    for j, alignment in enumerate(alignments):\n",
    "        entry = {}\n",
    "        # print('word alignment', alignment)\n",
    "        src_start = alignment[0][0]\n",
    "        # print('src_start', src_start)\n",
    "        src_end = alignment[0][1]\n",
    "        # print('src_end', src_end)\n",
    "        tgt_start = alignment[1][0]\n",
    "        # print('tgt_start', tgt_start)\n",
    "        tgt_end = alignment[1][1]\n",
    "        # print('tgt_end', tgt_end)\n",
    "        entry['id_sentence'] = i\n",
    "        entry['id_alignment'] = j\n",
    "        entry['query'] = src_sentences[i][src_start:src_end]\n",
    "        entry['context'] = tgt_sentences[i]\n",
    "        entry['answer'] = tgt_sentences[i][tgt_start:tgt_end]\n",
    "        entry['answer_start'] = tgt_start\n",
    "        entry['answer_end'] = tgt_end\n",
    "        char_check = entry['context'][entry['answer_start']:entry['answer_end']]\n",
    "        query_encoding = tokenizer(entry['query'])\n",
    "        context_encoding = tokenizer(entry['context'])\n",
    "        entry['answer_start_token'] = context_encoding.char_to_token(entry['answer_start']) + len(query_encoding['input_ids']) - 1\n",
    "        entry['answer_end_token'] = context_encoding.char_to_token(entry['answer_end']-1) + len(query_encoding['input_ids'])\n",
    "        \n",
    "        input_encoding = tokenizer(entry['query'], entry['context'],\n",
    "                                   padding='max_length',\n",
    "                                   max_length=128,\n",
    "                                   truncation=True\n",
    "                                   )\n",
    "        token_check = tokenizer.decode(input_encoding['input_ids'][entry['answer_start_token']:entry['answer_end_token']])\n",
    "        if not entry['query']:\n",
    "            print('query missing')\n",
    "        \n",
    "        if not char_check == ''.join((token_check).split()):\n",
    "            print('----------------------------------------------')\n",
    "            print(entry['id_sentence'], entry['id_alignment'])\n",
    "            print('src_sentences[i]', src_sentences[i])\n",
    "            print('query_start', src_start)\n",
    "            print('query_end', src_end)\n",
    "            print(\"entry['query']\", entry['query'])\n",
    "            print(\"entry['context']\", entry['context'])\n",
    "            print(\"entry['answer']\", entry['answer'])\n",
    "            print(\"entry['answer_start']\", entry['answer_start'])\n",
    "            print(\"entry['answer_end']\", entry['answer_end'])\n",
    "            print('########### char_check', char_check)\n",
    "            print('query_encoding', query_encoding)\n",
    "            print('context_encoding', context_encoding)\n",
    "            print(entry['answer_start_token'])\n",
    "            print(entry['answer_end_token'])\n",
    "            print('########### token_check', token_check)\n",
    "            print('########### ''.join((token_check).split())', ''.join((token_check).split()))\n",
    "            print('###########', char_check == ''.join((token_check).split()))\n",
    "\n",
    "        # test if the answer_start_token:answer_end_token is the same as the answer\n",
    "        # print(tokenizer.decode(tokenizer(entry['context'])['input_ids'][entry['answer_start_token']:entry['answer_end_token']]))\n",
    "        \n",
    "        # print(entry)\n",
    "        dataset_dev_list.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ec53eec0ca4feaa6948501affb6f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8a0f4dfaf42158e3fcf8faaaffe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id_sentence', 'id_alignment', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 37245\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id_sentence', 'id_alignment', 'query', 'context', 'answer', 'answer_start', 'answer_end', 'answer_start_token', 'answer_end_token', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3941\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_train = Dataset.from_list(dataset_train_list)\n",
    "dataset_dev = Dataset.from_list(dataset_dev_list)\n",
    "\n",
    "dataset = DatasetDict({'train': dataset_train, 'validation': dataset_dev})\n",
    "# print(dataset)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['query'], example['context'], \n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     max_length=128,\n",
    "                     )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'answer_start_token', 'answer_end_token'])\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(tokenized_dataset['train'], batch_size = 32, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(tokenized_dataset['validation'], batch_size = 32, shuffle = True)\n",
    "\n",
    "# inspect the first batch\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     print(batch['input_ids'])\n",
    "#     print(batch['token_type_ids'])\n",
    "#     print(batch['attention_mask'])\n",
    "#     print(batch['answer_start_token'])\n",
    "#     print(batch['answer_end_token'])\n",
    "#     print(batch.keys())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f49daa2b2448ac851443f4d8f910f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/food/food-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 1164 \n",
      "Loss: 3.8 \n",
      "\n",
      "Batch 20 / 1164 \n",
      "Loss: 3.3 \n",
      "\n",
      "Batch 30 / 1164 \n",
      "Loss: 3.0 \n",
      "\n",
      "Batch 40 / 1164 \n",
      "Loss: 2.8 \n",
      "\n",
      "Batch 50 / 1164 \n",
      "Loss: 2.4 \n",
      "\n",
      "Batch 60 / 1164 \n",
      "Loss: 1.9 \n",
      "\n",
      "Batch 70 / 1164 \n",
      "Loss: 0.8 \n",
      "\n",
      "Batch 80 / 1164 \n",
      "Loss: 0.7 \n",
      "\n",
      "Batch 90 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 100 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 110 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 120 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 130 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 140 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 150 / 1164 \n",
      "Loss: 0.7 \n",
      "\n",
      "Batch 160 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 170 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 180 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 190 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 200 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 210 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 220 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 230 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 240 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 250 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 260 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 270 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 280 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 290 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 300 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 310 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 320 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 330 / 1164 \n",
      "Loss: 0.9 \n",
      "\n",
      "Batch 340 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 350 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 360 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 370 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 380 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 390 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 400 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 410 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 420 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 430 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 440 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 450 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 460 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 470 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 480 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 490 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 500 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 510 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 520 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 530 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 540 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 550 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 560 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 570 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 580 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 590 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 600 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 610 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 620 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 630 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 640 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 650 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 660 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 670 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 680 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 690 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 700 / 1164 \n",
      "Loss: 0.7 \n",
      "\n",
      "Batch 710 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 720 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 730 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 740 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 750 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 760 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 770 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 780 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 790 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 800 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 810 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 820 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 830 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 840 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 850 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 860 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 870 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 880 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 890 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 900 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 910 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 920 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 930 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 940 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 950 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 960 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 970 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 980 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 990 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1000 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 1010 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1020 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1030 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 1040 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1050 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 1060 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1070 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 1080 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1090 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1100 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1110 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 1120 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1130 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1140 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1150 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1160 / 1164 \n",
      "Loss: 0.4 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d7a733e20a4be9b0cb3a987bf2f04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 124 \n",
      "Loss: 0.8 \n",
      "\n",
      "Batch 20 / 124 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 30 / 124 \n",
      "Loss: 2.1 \n",
      "\n",
      "Batch 40 / 124 \n",
      "Loss: 1.6 \n",
      "\n",
      "Batch 50 / 124 \n",
      "Loss: 1.2 \n",
      "\n",
      "Batch 60 / 124 \n",
      "Loss: 1.0 \n",
      "\n",
      "Batch 70 / 124 \n",
      "Loss: 1.1 \n",
      "\n",
      "Batch 80 / 124 \n",
      "Loss: 1.7 \n",
      "\n",
      "Batch 90 / 124 \n",
      "Loss: 1.3 \n",
      "\n",
      "Batch 100 / 124 \n",
      "Loss: 2.0 \n",
      "\n",
      "Batch 110 / 124 \n",
      "Loss: 2.0 \n",
      "\n",
      "Batch 120 / 124 \n",
      "Loss: 1.1 \n",
      "\n",
      "\n",
      "-------Epoch  1 -------\n",
      "Training Loss: 0.5213457851629077 \n",
      "Validation Loss: 1.5571457070689048 \n",
      "Time:  939.8729500770569 \n",
      "----------------------- \n",
      "\n",
      "\n",
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cff23a864a94e809331f36135692c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 20 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 30 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 40 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 50 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 60 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 70 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 80 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 90 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 100 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 110 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 120 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 130 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 140 / 1164 \n",
      "Loss: 0.7 \n",
      "\n",
      "Batch 150 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 160 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 170 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 180 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 190 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 200 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 210 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 220 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 230 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 240 / 1164 \n",
      "Loss: 0.7 \n",
      "\n",
      "Batch 250 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 260 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 270 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 280 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 290 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 300 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 310 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 320 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 330 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 340 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 350 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 360 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 370 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 380 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 390 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 400 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 410 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 420 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 430 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 440 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 450 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 460 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 470 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 480 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 490 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 500 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 510 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 520 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 530 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 540 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 550 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 560 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 570 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 580 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 590 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 600 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 610 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 620 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 630 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 640 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 650 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 660 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 670 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 680 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 690 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 700 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 710 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 720 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 730 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 740 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 750 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 760 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 770 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 780 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 790 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 800 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 810 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 820 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 830 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 840 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 850 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 860 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 870 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 880 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 890 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 900 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 910 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 920 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 930 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 940 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 950 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 960 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 970 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 980 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 990 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1000 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1010 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1020 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1030 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1040 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1050 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1060 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1070 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1080 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1090 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1100 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1110 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1120 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1130 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1140 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1150 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1160 / 1164 \n",
      "Loss: 0.1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc396059f91a4c478eeda16266e043a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 124 \n",
      "Loss: 2.3 \n",
      "\n",
      "Batch 20 / 124 \n",
      "Loss: 1.5 \n",
      "\n",
      "Batch 30 / 124 \n",
      "Loss: 2.5 \n",
      "\n",
      "Batch 40 / 124 \n",
      "Loss: 1.0 \n",
      "\n",
      "Batch 50 / 124 \n",
      "Loss: 1.2 \n",
      "\n",
      "Batch 60 / 124 \n",
      "Loss: 1.6 \n",
      "\n",
      "Batch 70 / 124 \n",
      "Loss: 1.2 \n",
      "\n",
      "Batch 80 / 124 \n",
      "Loss: 1.9 \n",
      "\n",
      "Batch 90 / 124 \n",
      "Loss: 1.0 \n",
      "\n",
      "Batch 100 / 124 \n",
      "Loss: 2.6 \n",
      "\n",
      "Batch 110 / 124 \n",
      "Loss: 2.4 \n",
      "\n",
      "Batch 120 / 124 \n",
      "Loss: 1.2 \n",
      "\n",
      "\n",
      "-------Epoch  2 -------\n",
      "Training Loss: 0.2645350722292009 \n",
      "Validation Loss: 1.6240741216367292 \n",
      "Time:  940.6285943984985 \n",
      "----------------------- \n",
      "\n",
      "\n",
      "############Train############\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5df71f96e341428c168fac9a2a8c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 20 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 30 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 40 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 50 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 60 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 70 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 80 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 90 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 100 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 110 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 120 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 130 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 140 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 150 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 160 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 170 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 180 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 190 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 200 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 210 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 220 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 230 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 240 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 250 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 260 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 270 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 280 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 290 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 300 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 310 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 320 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 330 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 340 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 350 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 360 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 370 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 380 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 390 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 400 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 410 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 420 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 430 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 440 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 450 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 460 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 470 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 480 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 490 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 500 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 510 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 520 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 530 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 540 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 550 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 560 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 570 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 580 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 590 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 600 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 610 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 620 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 630 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 640 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 650 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 660 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 670 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 680 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 690 / 1164 \n",
      "Loss: 0.6 \n",
      "\n",
      "Batch 700 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 710 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 720 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 730 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 740 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 750 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 760 / 1164 \n",
      "Loss: 0.5 \n",
      "\n",
      "Batch 770 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 780 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 790 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 800 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 810 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 820 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 830 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 840 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 850 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 860 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 870 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 880 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 890 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 900 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 910 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 920 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 930 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 940 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 950 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 960 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 970 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 980 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 990 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 1000 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1010 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1020 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1030 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1040 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1050 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1060 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1070 / 1164 \n",
      "Loss: 0.4 \n",
      "\n",
      "Batch 1080 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1090 / 1164 \n",
      "Loss: 0.1 \n",
      "\n",
      "Batch 1100 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1110 / 1164 \n",
      "Loss: 0.3 \n",
      "\n",
      "Batch 1120 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1130 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1140 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1150 / 1164 \n",
      "Loss: 0.2 \n",
      "\n",
      "Batch 1160 / 1164 \n",
      "Loss: 0.1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e85ed6584194d42a5fd573a40858617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 / 124 \n",
      "Loss: 1.6 \n",
      "\n",
      "Batch 20 / 124 \n",
      "Loss: 2.0 \n",
      "\n",
      "Batch 30 / 124 \n",
      "Loss: 1.8 \n",
      "\n",
      "Batch 40 / 124 \n",
      "Loss: 1.8 \n",
      "\n",
      "Batch 50 / 124 \n",
      "Loss: 2.2 \n",
      "\n",
      "Batch 60 / 124 \n",
      "Loss: 1.6 \n",
      "\n",
      "Batch 70 / 124 \n",
      "Loss: 1.7 \n",
      "\n",
      "Batch 80 / 124 \n",
      "Loss: 2.4 \n",
      "\n",
      "Batch 90 / 124 \n",
      "Loss: 0.9 \n",
      "\n",
      "Batch 100 / 124 \n",
      "Loss: 1.5 \n",
      "\n",
      "Batch 110 / 124 \n",
      "Loss: 2.7 \n",
      "\n",
      "Batch 120 / 124 \n",
      "Loss: 1.8 \n",
      "\n",
      "\n",
      "-------Epoch  3 -------\n",
      "Training Loss: 0.2253652645572636 \n",
      "Validation Loss: 1.873838396562684 \n",
      "Time:  940.7559235095978 \n",
      "----------------------- \n",
      "\n",
      "\n",
      "Total training and evaluation time:  2821.25829577446\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=1000)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "parallel_model = torch.nn.DataParallel(model)  # Use DataParallel\n",
    "optimizer = torch.optim.AdamW(parallel_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training setup\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "current_timeanddate = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# save validation statistics to a dataframe\n",
    "df = pd.DataFrame()\n",
    "# append to csv\n",
    "df.to_csv(f'{current_timeanddate}_{model_name}_train_val_loss.csv')\n",
    "\n",
    "epochs = 3\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Set parallel model in train mode\n",
    "    parallel_model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    print(\"############Train############\")\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch['input_ids'].to(device),\n",
    "            'token_type_ids': batch['token_type_ids'].to(device),\n",
    "            \"attention_mask\": batch['attention_mask'].to(device),\n",
    "            \"start_positions\": batch['answer_start_token'].to(device),\n",
    "            \"end_positions\": batch['answer_end_token'].to(device),\n",
    "        }\n",
    "\n",
    "        outputs = parallel_model(**inputs)\n",
    "        loss = outputs[0].mean()\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(\"Batch {:} / {:}\".format(batch_idx + 1, len(train_loader)), \"Loss:\", round(loss.item(), 1))\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    ########## Evaluation ##################\n",
    "    parallel_model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(val_loader), total=len(val_loader)): \n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch['input_ids'].to(device),\n",
    "                'token_type_ids': batch['token_type_ids'].to(device),\n",
    "                \"attention_mask\": batch['attention_mask'].to(device),\n",
    "                \"start_positions\": batch['answer_start_token'].to(device),\n",
    "                \"end_positions\": batch['answer_end_token'].to(device),\n",
    "            }\n",
    "            \n",
    "            outputs = parallel_model(**inputs)\n",
    "            loss = outputs[0].mean()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % print_every == 0:\n",
    "                print(\"Batch {:} / {:}\".format(batch_idx + 1, len(val_loader)), \"Loss:\", round(loss.item(), 1))\n",
    "\n",
    "    epoch_loss /= len(val_loader)\n",
    "    val_losses.append(epoch_loss)\n",
    "\n",
    "    print(\"\\n-------Epoch \", epoch + 1, \n",
    "          \"-------\"\n",
    "          \"\\nTraining Loss:\", train_losses[-1],\n",
    "          \"\\nValidation Loss:\", val_losses[-1],\n",
    "          \"\\nTime: \", (time.time() - epoch_time),\n",
    "          \"\\n-----------------------\",\n",
    "          \"\\n\\n\")\n",
    "    \n",
    "    # save validation statistics to a dataframe\n",
    "    df = pd.DataFrame({'train_loss': train_losses, 'val_loss': val_losses})\n",
    "    # append to csv\n",
    "    df.to_csv(f'{model_name}_train_val_loss.csv', index=False, header=True, mode='a')\n",
    "\n",
    "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/pgajo/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff951484c3334398a0ce7f46a157696e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pgajo/mdeberta-v3-base-xl-wa/commit/88dd90e267e82494408aeb146783e04e58e15330', commit_message='Upload DebertaV2ForQuestionAnswering', commit_description='', oid='88dd90e267e82494408aeb146783e04e58e15330', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login(token=\"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\")\n",
    "model.push_to_hub(f\"pgajo/mdeberta-v3-base-xl-wa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
